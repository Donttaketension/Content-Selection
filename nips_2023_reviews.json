{
  "Biologically-inspired adaptive learning in the Hopfield-network based self-optimization model": {},
  "Flexible Attention-Based Multi-Policy Fusion for Efficient Deep Reinforcement Learning": {
    "review_20": {
      "summary": "This paper defines the Knowledge-Grounded RL setting, a general RL setting for integrating knowledge (in the form of policies) into a policy to learn new tasks efficiently. Essentially this setting is similar to the MDP setting except that the agent is also given a set of knowledge policies to utilize. The paper also introduces a system/architecture within this KGRL setting, called Knowledge-Inclusive Attention Network (KIAN). The aim is to improve RL that is grounded on external knowledge policies. The paper outlines five desirable human-like properties they desire in their agents: knowledge-acquirable, sample-efficient, generalizable, compositional, and incremental. Moreover, they formally define these so that they are measurable within the KGRL setting (e.g., for evaluating algorithms on these dimensions).\n\nWhile previous methods typically intertwine knowledge representation and knowledge-fusion, thereby restricting their ability to adapt to numbers of policies, losing flexibility. KIAN is developed more flexibility, separating the knowledge representation and knowledge fusion.\n\nKIAN consists of three components: a policy that learns a strategy (similar to a normal policy) called the internal, embeddings that represent the given knowledge (or external) policies, a query that performs attentive action prediction to fuse the internal and external policies.\n\nKIAN also solves other issues that can occur in entropy-regularized KGRL. Entropy-regularized RL is common, but the authors show that in the KGRL setting issues can arise through entropy regularization where only a select few policies are selected, counterproductively reducing diversity in policy usage. The authors show that in the KGRL setting the agent will pay more attention to the policy with large entropy and in continuous control, will rely on the internal policy extensively. The paper introduces modifications so that this does not occur in KIAN.\n\nThe authors show results on both MiniGrid and robotics tasks and demonstrate sample efficiency, generalizability, as well as compositional and incremental learning.\n",
      "strengths": "The paper is mostly well-written and well-explained.\n\nThe method makes sense, and is a well-thought out architecture. \n\nI like how the authors address the entropy imbalance problem.\n\nI like that the authors define and quantify the behaviors they would like in the agent.\n\nThe results do seem to demonstrate their method is effective.\n",
      "weaknesses": "While I think the experiments are good, with appropriate baselines and good environments to test the agent\u2019s capabilities. I am concerned about statistical significance. In particular, only 5 seeds are run, and the performance benefit in many cases is minimal, which may quite possibly be attributed to randomness. While I do believe the method outperforms the baselines, I cannot say so with a lot of confidence to merit inclusion at NeurIPS. If the authors can run more seeds, especially given the large variance, it would dramatically improve their results.\n\n\nQualms:\nKGRL is consistently referred to as an RL framework, which it is, but the connotation can be misconstrued as being a framework \u201cfor\u201d RL, implying it is a solution method for RL problems. I would recommend calling it a \u201csetting\u201d rather than a framework. Indeed, I was confused temporarily as a reader, especially when KGRL is stated as being \u201cintroduced\u201d by this paper (as opposed to \u201cdescribed\u201d or \u201coutlined\u201d). \n\n\nNits\nTypo Line 345: \u201cborder\u201d should be: \u201cbroader\u201d\n"
    },
    "review_21": {
      "summary": "Humans can learn by aggregating external knowledge from others\u2019 policies of attempting a task. While prior studies in RL have incorporated external knowledge policies for sample efficiency,  there still remains the generalization problem to be solved that agents have difficulties to perform arbitrary combinations and replacements of external policies.\nThe authors propose a new actor architecture for Knowledge-Grounded RL (KGRL),  Knowledge-Inclusive Attention Network (KIAN), which allows free knowledge rearrangement due to embedding-based attentive action prediction.  KIAN addresses entropy imbalance as well. The authors demonstrate in experiments that KIAN outperforms other methods incorporating external knowledge policies under different environmental setups.",
      "strengths": "The authors clearly define the problem as how RL can be grounded on any given set of external knowledge policies to achieve knowledge-acquirable, sample-efficient, generalizable, compositional, and incremental properties.\nThe proposed method of KIAN is clearly described. They use knowledge keys and the query performs an attention operation to determine how an agent integrates all policies. \nThe solution of the entropy imbalance issues when integrating external policies are proposed as well.",
      "weaknesses": "Generally this work has many related works but is tackling the unique challenge problem of fusing knowledge policies with different state and action spaces.\nLimitations of the proposed method is not clear based on the experimental results."
    },
    "review_22": {
      "summary": "The authors of this work introduce Knowledge-Grounded RL (KGRL), an RL framework that combines multiple knowledge policies to achieve human-like efficiency and flexibility in learning. They propose a new actor architecture called Knowledge-Inclusive Attention Network (KIAN) that enables arbitrary combinations and replacements of external policies through embedding-based attentive action prediction. KIAN also addresses entropy imbalance, a challenge in KGRL, addressing exploration in the environment.",
      "strengths": "Firstly, the paper addresses a relevant and interesting problem in the field of reinforcement learning (RL) by improving sample efficiency from arbitrary external policies and enabling knowledge transfer skills. \nFurthermore, the paper is a well-written work that effectively conveys the ideas and findings in a clear and concise manner. The authors demonstrate excellent writing skills, employing appropriate terminology and organizing the content in a manner that enhances readability and comprehensibility.\nIn addition, the paper effectively establishes the motivation and position of the study in the existing literature. The authors articulate the significance and relevance of the research problem, demonstrating a strong understanding of the field. They situate their work within the broader scholarly context, highlighting how their study fills a gap in knowledge or builds upon prior research.\nThe methodology employed in the research is clearly described, allowing readers to understand and replicate the study. The authors provide a detailed and comprehensive explanation of the experimental and theoretical approach used, supported by well-designed figures and diagrams. These visual aids enhance the understanding of the methods employed and facilitate better comprehension of the research. Lastly, the authors provide solid mathematical understanding for their proposed method. \n\nCollectively, these strengths highlight the quality of the paper. ts focus on addressing a relevant and interesting problem, combined with its well-written content, clear methodology, positioning in the literature, and mathematical rigor, make it an intersting contribution.",
      "weaknesses": "The paper has one notable weakness that should be addressed to enhance the overall quality. The evaluation presented in the paper is quite limited. The results obtained for the OpenAI robotics task only offer a slight support for the proposed method. The authors should consider expanding the evaluation to provide a more comprehensive assessment of the proposed method's performance.\nAdditionally, the use of only 3 or 5 seeds in the experiments may be insufficient, especially when considering the large error bars observed in some models. The authors should consider increasing the number of seeds to improve the statistical robustness of the results.\nMoreover, the paper lacks clarity regarding the error bars shown in the plots. It is not explicitly stated what these error bars represent, which hinders the interpretation and understanding of the presented data.\nLastly, the paper would greatly benefit from conducting ablation studies to investigate the effects of various factors on the proposed method's performance. Specifically, the authors could consider performing ablation studies on \n- the influence and distribution of attention/weights of actors, \n- the impact of random/irrelevant policies in $\\mathcal{G}$,\n- the impact of the (near) optimal policy in $\\mathcal{G}$, \n- the impact of a larger set of knowledge policies $\\mathcal{G}$, \n- the effects of different types of knowledge policies, \n- and an investigation of the mentioned entropy balance issue, that the authors specifically address in their method.\nThese ablation studies would provide valuable insights into the individual contributions and impacts of these factors on the overall approach.\n\nOverall, addressing these weaknesses would significantly improve the research paper, clarifying important aspects such as error bars, conducting relevant ablation studies, ensuring consistency in reporting variances, and strengthening the empirical evidence for the proposed method.\n\n### Minor\n- I assume ori-KIAN is KIAN with the original $\\mathcal{G}$, this should be mentioned in the text. \n- l.314 mentions less variance, but tables do not show variances for individual experiments"
    },
    "review_23": {
      "summary": "This paper introduces KIAN, a method for leveraging multiple sub-optimal policies to solve reinforcement learning tasks. The paper starts out by introducing a new Knowledge-Grounded MDP, which adds a set of external knowledge policies to the traditional MDP framework. \n\nTo leverage these external knowledge policies, KIAN learns an embedding for each knowledge policy (including an inner knowledge policy for the current agent). Then at each step, the current state is mapped to a query embedding that can be used to find the policy that is best equipped to take an action at that timestep.",
      "strengths": "* The paper is structured well and is easy to read.\n* The authors demonstrated great attention to detail by motivating the theorems and equations with intuition before defining them concretely.\n*  The paper is very technically sound.",
      "weaknesses": "* > In such a case, whenever the knowledge set is updated by adding or replacing policies, prior methods require relearning the entire multi-policy fusion process, even if the current task is similar to the previous one. This is because their designs of knowledge representations are intertwined with the knowledge-fusing mechanism, which restricts the number of policies in the knowledge set from being changed.\n\n    * It would be good to point to specific prior works that suffer from this problem so the reader can build intuition.\n\n* There are no visualizations of the tasks used for evaluation. Adding pictures of the environments would help readers understand what the agent needs to do."
    }
  },
  "Conditional Matrix Flows for Gaussian Graphical Models": {
    "review_8": {
      "summary": "This paper concerns the estimation of precision matrix under $l_p$ norm sparcity penal. The solution is a variational inference through normalizing flow, which is a function of shrinkage parameter $\\lambda$ and non-negative norm parameter $p$. It allows for straightforward computation of solution paths for the intervals of $\\lambda$ and $p$, and was empirically evaluated on two relatively small data sets.",
      "strengths": "Framework for GGM estimation based on conditional normalizing flows, indeed appears novel. Supporting math seems solid. \n\nUsing simulated annealing algorithm to recover a path of solutions for varying $\\lambda$ and $p$ is useful, in particular for the case of $p$, as in case of $\\lambda$ it was fairly straightforward to perform it with other methods too. I am just wondering how costly and scalable it is under the new framework, an empirical/theoretical analysis would be appreciated.",
      "weaknesses": "Empirical evaluation appears limited. It does not contain comparison with other (e.g. frequentist) approaches to derive the solution paths. Both in terms of estimation accuracy and in terms of computational cost."
    },
    "review_9": {
      "summary": "This paper targets the structure learning problem in Gaussian Graphical Models via (Normalising) Flow-based Variational approximation of the elements of weight metrics that correspond to the Gaussian Bayesian network. \nThey use sub-l1 pseudo norms to penalize dense precision metrics (which correspond to graphs with numerous links) without imposing an extra high penalty for large non-zero values (which typically occurs if $l_{\\geq1}$ is used).",
      "strengths": "1. Up to my knowledge, this is the first time flows are applied to the space of positive definite matrices. \n2. The proposed approach is flexible meaning the class of applicable prior and likelihood functions is quite large.\n3. Using sub-l1 norm is suitable for structure learning. \n4. The proposed algorithm is mathematically sound (as far as I can follow) and is quite interesting. \n5. The paper is well-written, and the relevant work is sufficiently discussed.    \n6. Due to its flexibility, the proposed method has the potential of having a large impact.",
      "weaknesses": "Due to the factors mentioned in the previous section, I find this work impressive and beautiful. However, unfortunately, the carried out experiments are minimal. Most notably, the algorithm is compared to no alternative work (neither in the main paper nor in the supplementary material). With no quantitative comparisons, it is impossible to evaluate the performance of the proposed algorithm compared to the existing methods. \n\nNOTE: In the Rebuttal, some experiments are carried out (though the code is still not accessible).    \n\nMinor suggestion: \n1. Though it is clear in the context, I suggest that the authors do not use the same letter \"p\" (with the same font) for both probability density and norm parameter.  \n2. Fix minor typos e.g. the end sentence period in line 214."
    },
    "review_10": {
      "summary": "This paper proposes a method that can be used to infer conditional independencies in a Gaussian model. These conditional independencies are related to zeros in the precision matrix. Typically, sparse enforcing norms are used to estimate the precision matrix while enforcing zeros in the elements outside of the diagonal. In this paper a Bayesian approach is considered. For this a pseudo-distribution for the data is considered by taking the exponential to the p-norm. The method is trained via variational inference combined with normalizing flows to increase the accuracy of the posterior approximation. The variational distribution is tuned via simulated annealing and a temperature parameter allows to interpolate between the Bayesian and the Map solution.",
      "strengths": "- Well written paper.\n\n        - Illustrative toy experiments.",
      "weaknesses": "- The proposed method is a combination of already known techniques.\n\n        - The experimental section is weak as only a single real problem is considered.\n\n        - Although the proposed method is a generalization of several known techniques, I have found in the experimental section a lack of comparisons with other related methods.\n\n        My main point of criticism is the weak experimental section which only considers a single real problem and no comparisons with other related methods are carried out in real problems.\n\n        Another point of criticism is that, for some particular values of the p parameter one does not actually observe sparsity in the Bayesian solution. For example, when sampling from the Laplace distribution one never observes zeros in practice. Spike and slab priors (a mix between a Gaussian and a point of mass center at zero) are the ones that actually lead to zeros."
    },
    "review_11": {
      "summary": "This work proposed a framework for performing inference on Gaussian Graphical Models by approximating the posterior with a normalizing flow over PSD matrices. In this way, the authors can investigate $l_p$-norm regularized GGMs for any value of $p$ in an efficient way.",
      "strengths": "The idea of using normalizing flows for GGM inference definitely brings in advantages of both Bayesian and frequentist worlds; to me, that's an innovative idea.",
      "weaknesses": "The main weakness that I identified is the lack of comparison between the proposed framework and the well-studied graphical lasso with concave approximations of the $l_0$-norm. More precisely, the authors show that their framework obtains frequentist solution paths through simulated annealing, therefore, it'd be of great interest to see a comparison between these solution paths and those obtained by iterative algorithms such as iterative reweighted l1-norm for graphical lasso."
    }
  },
  "Matrix Compression via Randomized Low Rank and Low Precision Factorization": {
    "review_23": {
      "summary": "The paper introduces a low rank, quantized/low precision matrix factorization which decomposes an n x d matrix A in the form A= LR, where L (of size n x m) and R (of size m x d) are low rank factors. L and R are computed using a random projection matrix S in the form L = Q(AS) and R = Q'(W^*) where W^* is the matrix minimizing the squared Frobenius norm ||Q(AS)W\u2212 A||. Q and Q' are two independent quantizers with specified budgets. \n\nThe authors contrast their method with an SVD based method for computing the quantized low rank approximation which instead sets L = Q(U_k S_k) and R = Q'(V_k), where U_k/V_k and S_k are the singular vectors/values respectively. The paper has a theorem deriving a bound on the Frobenious norm of the factorization eror. They apply the approximation on image data (for image compression) and embedding matrices (for an embedding classifation task).\n",
      "strengths": "+ The paper is very well written and very clear in its presentation\n+ Clear technical presentation incl. theorems, algorithms etc.\n+ Novel idea, providing good review of relevant literature (different matrix sketching approaches, quantization etc.)\n+ Thoughtful experiments to demonstrate real world application (using embedding compression)\n",
      "weaknesses": "I don't see any weaknesses that need to be addressed at this moment."
    },
    "review_24": {
      "summary": "The paper studies the low-rank factorization of the matrix in the low-precision setting and proposes a new algorithm which is a combination of randomized low-rank approximation method and quantization.  The paper formally analyzes the guarantee of the proposed algorithms and also give experiments on real world dataset which demonstrate the advantage of the proposed algorithm.",
      "strengths": "1. The presentation of the paper is good. The writing of the paper is clear and easy to follow.\n\n2. To get the formal guarantee, the authors do a careful analysis, which I think is non-trivial.",
      "weaknesses": "1. Technical novelty: the main algorithm (Algorithm 1) seems to just be the standard way in randomized numerical linear algebra then plusing the quantization. Can you authors give more explanations about the technical novelty? (though the analysis I think is not standard)\n\n2. Experiment: I have some questions about the setting and details of the experiments section. See the next question."
    },
    "review_25": {
      "summary": "The paper proposed a memory efficient approach to approximate a matrix $A$ by: low-rank approximation $A=LR$ and quantization. The LPLR algorithm first applies a quantized random projection (RP) as the $L$, and then solve a minimization problem for the right loew-rank factor $R$, which is also quantized afterwards. Theoretical approximation error is obtained and compared with an alternative approach that quantized SVD low rank factors instead of RP. Experiments are conducted on image approximation and embedding classification, to show the effectiveness of the proposed method.",
      "strengths": "1. The paper is well-organized and easy to follow. The theoretical analysis seems rigorous.\n\n2. Experiments on multiple ML tasks and datasets are provided which make the results more grounded.",
      "weaknesses": "In my understanding, the main idea of the paper is to waive the need to compute the SVD of A, by using random projection (RP) as a surrogate. I have the following concerns and suggestions:\n\n1. At line 220, the authors wrote that the bits per entry for SVD-quant is $O(nd\\sqrt k)$, but in Table 1 and Table 2, it is $O(k\\sqrt{nd})$. Please double check and clarify. Also, the authors simply stated that LPLR is better than SVD-quant in terms of bits per query, which is not true with some n, d, m, k (comparing the results in the table). I suggest the authors to carefully compare the results and state the regimes when LPLR is better, and when it is worse.\n\n2. In the main Theorem 3.2, $\\kappa$ could be negative, right? Is $1-c_4\\sigma_k/\\sigma_{k-1}$ bounded? Or do we need to further assume an eigen gap for this result to hold?\n\n3. I understand that the main usage of LPLR is for matrix (data) approximation, so the first experiments (Figure 1 and Table 3) make sense to me. However, for the second set of experiments on classification, why not directly using $Q(AS)$ (i.e., the quantized random projections)? This saves the storage for W (in other words, we may increase the sketch size m when using Q(AS) only). Some recent references on this include\n\nRandom projections with asymmetric quantization, Li and Li NeurIPS 2019\n\nGeneralization error analysis of quantized compressive learning, Li and Li, NeurIPS 2019 \n\nIndeed, the research on QRP is highly related to this submission, since LPLR essentially does an optimization on W to recover the data from QRP. I suggest to add some discussion on this direction in the paper and some empirical comparisons.\n\n4. Also, if LPLR is used for processing or storing the data for classification or search tasks, it might be inconvenient to handle new data points (e.g., in a streaming setting). Thus, it may not be suitable for such tasks. On the other hand, recently people are using low rank approximation in LLM fine-tuning frequently. Experiments related to fine-tuning language models could be a better application scenario for the proposed method.\n\n5. Some references on similar results are missing. A similar result as in Appendix D that $S^TQ(AS)$ with uniform quantization has approximation error independent of $d$ has been established in [EDEN: Communication-Efficient and Robust Distributed\nMean Estimation for Federated Learning, Vargaftik et al., ICML 2022] (or maybe some even earlier paper) for rotation matrix $S$. This related result should be cited. Also, Eq. (2) is a standard result for uniform stochastic rounding. A reference should be added there.\n\nIn all, I think the paper proposes a simple but intuitive method for low-rank low-precision matrix approximation from QRPs. The idea is clear, the analysis seems sufficient (despite the above and below questions).The experiments can be improved, but the current results on several tasks and datasets are convincing enough to show the effectiveness of LPLR in matrix approximation. For now, I would recommand borderline accept. "
    },
    "review_26": {
      "summary": "The paper studies compression of low-rank matrices by simultaneous low-rank factorization and quantization. It proposes a method that first quantizes the randomized rangefinder as the first low-rank factor and then quantize the minimizer of reconstruction error with respect to the remaining factor as the second. Randomized rangefinder uses random Gaussian matrix which possesses the equalization property to maintain low quantization error, compared to na\u00efve quant. Experiments are provided to demonstrate the benefits of the proposed algorithm.",
      "strengths": "1. provide a low rank factorization algorithm that come with quantization for further reducing memory footprint.\n2. experiments demonstrate the advantage of the algorithm.",
      "weaknesses": "1. experimental settings should be made clearer. the current description is a bit confusing. \n"
    },
    "review_27": {
      "summary": "The authors investigate combining low-rank matrix factorization and (uniform scalar) quantization.\nThrough theoretical analysis and experiments they demonstrate that this can yield much higher accuracy than directly quantizing the input matrix. One natural choice is to compute the SVD of the matrix and quantize the two factors independently. It's shown that quantizing the left factor first and then computing a new right factor that best approximates the input when multiplied with the quantized left factor yields much better result. The choice of SVD is not critical and could be replaced by randomly mixing the columns of the input matrix, i.e. random sketching. In fact sketching has provable benefits as the entries of sketched matrices are bounded with very high probability. This bounded range improves quantization theoretically. The authors prove several theorems (their proofs are in the extensive appendix) and conduct detailed empirical evaluation.",
      "strengths": "1) All the key ideas of the paper (low-rank approximation, sketching) are sound.\n2) Detailed theoretical analysis with rigorous proofs.\n3) Extensive experiments.\n4) Compression of neural network weights and embeddings via low rank approximation and quantization are popular and impactful topics both to reduce memory usage and to speed up training and inference.\n",
      "weaknesses": "1) Neither the ideas nor the analysis are particularly inventive in my opinion. It's self evident that optimizing the second factor after quantizing the first (LSVD) is superior to independent quantization (DSVD), in fact the process could be iterated further. While I appreciate the 30+ pages of proofs provided by the authors it seems as if they rely on chaining known results and techniques for sketching and random matrices combining with patient algebra.\n2) Despite its seemingly weaker theoretical bounds LSVD is always one of the most accurate method in the experiments (see Tables 4-7). This is also clearly highlighted by the authors in the limitations section. \n3) Quantization aware training (not considered in the paper) is highly likely to produce equivalent or better results.\n"
    },
    "review_28": {
      "summary": "This paper introduces a novel low-rank matrix factorization algorithm that is using sketching matrix idea and quantization, such as they do:\n\n1. Use Gaussian RV to generate sketch of the matrix and compute the approximate basis\n2. Use Quantization with Q - to get Q(AS)\n3. Use Q(AS) and Q' to get  Q'(W)\n4. Return Q(AS) and Q'(W)\n\nAuthors provide theoretical and numerical analysis of their idea.",
      "strengths": "- introduction section written very well & and very informative and to the point, such as introducing LPLR algorithm briefly, talking about Low-rank approximation and Randomized quantization.\n\n- the introduced algorithm clearly communicated and results are theoretically sound.",
      "weaknesses": "- Abstract seem to be a bit wordy - it would be nice if there were formulations and numbers that grabs readers attention with resutls.\n\n- i like the way motivating the work - with memory constraints - but i am curious is there any application in real life for low rank decomposition that actually saves memory. some examples would be good\n\n- the paper doesn't exactly introduce a new direction - rather seems to be using existing ideas and put them together."
    },
    "review_29": {
      "summary": "This work studies the problem of computing a low rank approximation when the low rank factors are under a bit budget constraint, that is, we must output factors L and R with bounded bits such that LR approximates a given input matrix A in the Frobenius norm. The authors show that by incorporating sketching into the quantization procedure, one can get improved bounds, due to the fact that a Gaussian sketch can \u201cflatten\u201d the entries of a vector, which is advantageous when rounding (Appendix D). Empirical results show that this algorithm indeed gives improved results over other naive implementations such as directly rounding SVD factors. ",
      "strengths": "The problem of efficiently quantizing low rank approximations is an extremely important problem given that both low rank approximations and low precision is gaining popularity for compressing massive neural networks (https://papers.nips.cc/paper/2020/file/13b919438259814cd5be8cb45877d577-Paper.pdf, https://arxiv.org/abs/2302.03764). This work offers an interesting new method which takes advantage of the \u201cflattening\u201d property of Gaussian sketches in order to obtain improved results for quantization in the context of low rank approximation. This idea is conceptually simple yet interesting. Empirical results are also convincing.",
      "weaknesses": "The contribution is already quite nice, but there are several followup investigations that could strengthen this work much more:\n* Are there any lower bounds on the trade-off between the approximation accuracy and the bit budget?\n* There are structured sketching transforms such as the Subsampled Randomized Hadamard Transform (see Theorem 2.4 of http://www.cs.cmu.edu/afs/cs/user/dwoodruf/www/wNow3.pdf) which also have the \u201cequalizing\u201d or \u201cflattening\u201d type of behavior, yet can be applied in much faster time, and furthermore also save on the storage of the sketching matrix since it is an integer matrix. Can this be used to get faster implementations in theory/practice? \n* Can you report the running time of the experiments?\n* Can you comment on whether the bit bounds imply actual savings in the memory usage, or are the practical implementations of bit complexity too crude to capture these improvements? "
    }
  },
  "Color Equivariant Convolutional Networks": {
    "review_15": {
      "summary": "This paper proposes color equivariant convolutional networks (CE-CNNs), a novel convolutional neural network architecture that achieves equivariance to hue changes.&#x20;\n\nThey introduce color equivariant convolutions that apply a discrete set of hue rotations to the convolution filters during the forward pass. This makes the network output invariant to corresponding hue changes in the input image.\nThey propose a group coset pooling layer that pools feature maps over the group of hue transformations to achieve invariance.\nThey evaluate CE-CNNs on several image classification datasets, showing improved robustness to hue changes at test time compared to regular CNNs. The method also improves absolute classification performance in some cases.\nOverall, the paper presents a novel and intuitive technique to build invariance to hue changes into CNNs. The evaluations demonstrate its advantages over standard networks, especially under shifted hue distributions between training and test.",
      "strengths": "This paper introduces a clever yet intuitive technique to make convolutional neural networks invariant to hue changes in the input image. The core idea is to apply discrete hue rotations to the convolution filters during the forward pass, essentially \"baking in\" robustness to color changes.\n\nThe paper is clearly written and easy to follow. The authors motivate the problem well, explain their proposed method succinctly, and provide thorough experimentation across image datasets. The visualizations offer useful insights, confirming that the networks learn consistent features across hues.\n\nOverall, I found this to be an original and significant contribution. Invariance to hue shifts is a practical problem, and this paper tackles it through an elegant approach that outperforms regular CNNs. The concept of encoding transformations into convolutions seems powerful. While not the flashiest technique, the method is thoughtful, principled, and achieves strong results. The paper is presented clearly and comprehensively, making the ideas accessible. In summary, this is a high quality paper with both theoretical and practical value.",
      "weaknesses": "-   The method is demonstrated on image classification, but it's unclear how well it would generalize to other tasks like detection or segmentation. Additional experiments on other applications could strengthen the claims.\n-   The ablation study on number of hue rotations suggests performance varies across different shifts. It would be useful to dig deeper into why - is it an artifact of how shifts are applied? Better understanding this could improve results further.\n-   The approach encodes discrete hue rotations. An interesting extension could be supporting continuous rotations for finer-grained equivariance.\n-   The comparisons to \"grayscale\" networks should be interpreted carefully, as removing color information entirely handicaps models. Comparisons to networks pre-trained onImagenet may be more meaningful.\n-   The Flowers-102 experiments indicate the method doesn't help much on datasets without color bias. Analyzing when color equivariance helps or hurts could guide adoption."
    },
    "review_16": {
      "summary": "Paper proposes color-equivariant CNN layers by imposing equivariance to H_n (a discrete subgroup of SO(3)) in the RGB space which is imposes hue equivariance. Implementation follows the framework of Group-equivariant CNNs. Experiments show marginal improvements over standard CNNs for in-distribution test data but significant improvements when test data is hue-shifted.",
      "strengths": "1. Color equivariance in CNNs is a relatively less-studied but an important topic for robustness. The proposed idea of incorporating equivariance to hue transformations via rotations in the RGB space is novel. \n2. Experiments are setup well clearly showing when color equivariance is helpful vs color invariance vs no symmetry. Proposed approach shows improves over CNN even when in in-distribution test data, but major improvements come when test data is hue-shifted.",
      "weaknesses": "1. Definition of color equivariance considered in the paper seems to be restricted as it only considers the hue dimension. One of the motivations for incorporating color equivariance is for robustness to illumination changes which I do not think is guaranteed here. A general definition of color-equivariance should consider other dimensions. Maybe the claims are better justified if Hue-equivariance is emphasized in the title/introduction/method name, etc.\n2. The definition of hue-equivariance is not precise in the paper. Ideally, it should include all rotations in the RGB space (i.e., SO(3)), but also consider the fact that many of these rotations take the color values out of the RGB space (unit cube). In general, this issue occurs for the discrete subgroup $H_n$ as well. Simply projecting the color values back into the RGB space does not work as it breaks the invertibility property of these transformations. \n3. Experiments compare with a standard CNN (+grayscale) as baseline. Other baselines can be included, for example [1], that considers invariance to illumination color/intensity. \n4. Experiments in the main paper only consider the group $H_3$ (i.e., 3 rotations in the RGB space), which seems limited in robustness, as shown in Figure 1 without jitter augmentations. \n\n\nReferences:\n\n[1] Lengyel, Attila, et al. \"Zero-shot day-night domain adaptation with a physics prior.\"\u00a0_Proceedings of the IEEE/CVF International Conference on Computer Vision_. 2021."
    },
    "review_17": {
      "summary": "The authors introduce a color equivariant convolutional neural network. To achieve this the authors represent the image in HSV format, and achieve hue equivariance using methods for rotational equivariance. This is possible since hue can be represented by an angle. The authors show that the proposed approach out performs standard CNNs and color invariant CNNs when there is a hue shift between the train and test set.",
      "strengths": "* Originality: The presented method of building a color equivariant CNN appears to be original. \n* Quality: The work appears to be of fairly good quality. \n* Clarity: The paper is well written. \n* Significance: The observation that color equivariance can be achieved by identifying hue with the rotation group is interesting. The results show the proposed approach leads to improved performance when there is a color based domain shift.",
      "weaknesses": "* Quality: I have some questions about the mathematical presentation, and experiment design (see questions).\n* Clarity: Some aspects were unclear to me, due to presentation or motivation (see questions)"
    },
    "review_18": {
      "summary": "This paper questions the importance of color variations for neural classifiers and proposes to use color-equivariant architectures in the case of unbalanced datasets.\nTo demonstrate the validity of the presented approach, the authors conduct experiments both on synthetic controlled datasets and on common object recognition benchmarks.\nAs the experiments show, the injection of color-equivariant layers leads to a slight improvement on almost all common benchmarks when the performance is measured on the original test set but the advantage of the presented method becomes more evident when the test data is corrupted with hue shifts.",
      "strengths": "This paper studies an interesting and underinvestigated question of the importance of color representation for neural networks.\nThe submission is easy to read, and the motivation is well explained in the example of the Flowers dataset. \nThe authors have conducted a significant number of experiments to support their claims.\nAdditional strength is that the demonstrated performance improvement is achieved without increasing the number of trainable parameters (line 238).",
      "weaknesses": "1. While the idea of extending equivariance from geometric to photometric transformations is definitely interesting, the submitted manuscript, unfortunately, focuses on the only type of such transformations, i.e. hue shifts. Despite the case of the Flowers dataset is a perfect fit for this transformation, the authors do not discuss other use cases when this type of equivariance may be interesting in practice and just mention \"accidental recording conditions\" (line 3). For other datasets, hue shifts seem less meaningful, and the better robustness of the proposed CE-ResNets to such shifts at test time is explained by the fact the architecture was just intentionally designed for this scenario. Taking this into account, I find the scope of the paper a bit limited.\n\n1. In addition to being limited in the number of considered photometric transformations, the paper also considers a single task of object recognition. I would encourage the authors to consider other tasks as well, e.g. unsupervised domain adaptation.\n\n1. While the authors claim their approach makes networks more robust to test time corruptions (Tab. 1), they do not demonstrate other baselines aiming to provide robust outputs, e.g. adversarially robust models."
    }
  },
  "VideoComposer: Compositional Video Synthesis with Motion Controllability": {
    "review_13": {
      "summary": "The paper presents a method for compositional video synthesis. It introduces motion vectors from compressed videos as a control signal for temporal dynamics. The motion vector can be combined by other conditions such as sketch, and depth map. Both qualitative and quantitative results show that the proposed method can control the spatial-temporal patterns. ",
      "strengths": "+ The motion controlled generation result (fig 8) using hand-crafted strokes is interesting. \n\n+ Table A1 shows effectiveness of the proposed method quantitatively compared to previous methods.\n\n+ The paper is well written and easy to follow.",
      "weaknesses": "- There are a few GAN-based video synthesis approaches that are worth discussing in the related work. For example, MoCoGAN [1] approaches the problem by decomposing motion and content. \n \n- The two-stage training strategy needs more clarification. What is \"compositional training\" particularly in the second stage?  How does it differentiate from the \"text-to-video\" generation in the first stage?\n\n- In line 164-165, the authors \"repeat the spatial conditions of a single image and single sketch along the temporal dimension\". If the input condition is simply repeated, what's the point of applying a temporal Transformer? It will be equivalent to applying the spatial operation only and repeat at the latent space but with higher computation cost, no? (for motion vector, I totally agree that a spatial-temporal modeling would be necessary.)\n\n- Motion vectors can be less meaningful in the background due to lack of high-level semantics. It can also be clearly seen from the top row in Fig 4. I wonder if the authors treat the motion vector field equally for all locations. It seems that the generated results with motion conditions has more blurry background.\n\n- From Figure 2 and Figure 1(d), my impression is that the conditions (say motion and depth) can be combined together. However, in ablation studies (table 2), only one condition is added at a time. Another ablation that studies all combinations of these conditions will be favored. \n\n[1] Tulyakov, Sergey, et al. \"Mocogan: Decomposing motion and content for video generation.\" CVPR 2018."
    },
    "review_14": {
      "summary": "VideoComposer is a tool designed to enhance video synthesis by incorporating textual, spatial, and temporal conditions. It uses motion vectors from compressed videos to guide temporal dynamics and employs a Spatio-Temporal Condition encoder to effectively integrate spatial and temporal relations of inputs. This improves inter-frame consistency and allows for greater control over the synthesized video's spatial and temporal patterns.\n",
      "strengths": "The VideoComposer offers better control over video synthesis, temporal guidance using motion vectors, improved inter-frame consistency with its Spatio-Temporal Condition encoder, versatility in accepting various forms of inputs, and high customizability, resulting in more precise and desired synthesized videos.",
      "weaknesses": "An ablation study could be conducted on VideoComposer, where each component is removed in turn to evaluate its impact on overall performance. This would help evaluate the value of training under multiple conditions versus a single condition. \n\nAdditionally, comparing VideoComposer to a simpler method like Text2Video-Zero [a] with ControlNet [b] would demonstrate whether the increased complexity of VideoComposer yields significantly better results, hence justifying its sophistication. \n\n[a] Text2Video-Zero: Text-to-Image Diffusion Models are Zero-Shot Video Generators, L Khachatryan et al.\n[b] Adding Conditional Control to Text-to-Image Diffusion Models, L. Zhang et al. "
    },
    "review_15": {
      "summary": "This work proposes a new method called VideoComposer for conditional video generation, especially for video-to-video translation. VideoComposer is constructed upon the Video Latent Diffusion Model and introduces an STC-encoder to integrate multiple spatial and temporal conditions such as RGB images, sketches, motion vector sequences, etc. The architecture design involves simple 2D convolutions and temporal transformer layers. The conditional features are fed into the U-Net input together with noise. The demonstrated results have good temporal consistency.",
      "strengths": "- This is one of the pioneering works in controllable video synthesis. The temporal consistency of the video results is impressive, considering that its conditioning modeling enables several editing abilities such as image-to-video translation and motion/depth/sketch-driven local/global video editing.\n\n- The jointly training strategy is good for flexible inference within one model, e.g., video inpainting, without second training.\n\n- The paper organization and illustrations are easy to follow.",
      "weaknesses": "- The authors could have tried other design choices for integrating Condition Fusion as input into the U-Net, such as integration through cross-attention.\n\n- In line 215, it is claimed that \u201cwe observe that the inclusion of mask and style guidance can facilitate structure and style control.\u201d However, the corresponding evidence should be presented for the style representation extracted by clip image encoder and concatenated with text embedding.\n\n- It seems that a single STC-encoder is used for all different conditions via random dropout. It would be interesting to see if different STC-encoder weights for different conditions are better.\n\n- The examples in Figure 6 with reference image look like failure cases. Besides, the tiger texture and box shape are changed in Figure 8. It would be helpful to see more discussion and analysis on this part.\n\n- The ablation study of STC-encoder is not presented in a fair way. The main benefit of using STC-encoder comes from the video information condition instead of the network design.\n\n- The important comparisons and discussions with other methods are not sufficient, such as VideoP2P and vid2vid-zero mentioned in the related works."
    },
    "review_16": {
      "summary": "This work aims to allows users to flexibly compose a video with textual conditions, spatial conditions, and temporal conditions.\nIt introduces a novel framework namely VideoComposer based on the paradigm of compositional generation.\nTo be specific, it introduces the motion vector from compressed videos as an explicit control signal to provide guidance regarding temporal dynamics.\nMoreover, it develop a Spatio-Temporal Condition encoder (STC-encoder) that serves as a unified interface to effectively incorporate the spatial and temporal relations of sequential inputs, with which the model could make better use of temporal conditions and hence achieve higher inter-frame consistency. \nExtensive experiments demonstrate that VideoComposer control the spatial and temporal patterns simultaneously within a synthesized video in various forms.",
      "strengths": "1. It introduces motion vector as a more flexible user-guided signal.\n2. It proposes Spatio-Temporal Condition encoder (STC-encoder) that serves as a unified interface to effectively incorporate the spatial and temporal relations of sequential inputs.\n3. Extensive experiments show the effectiveness and superiority of VideoComposer.",
      "weaknesses": "1. What is the difference between the roles of the ``Style`` of CLIP and ``Single Image`` of STC-encoder? They both seem to provide content to videos.\n2. VideoComposer only obtain comparable performance with prior video generative models. Is it more efficient than previous methods? The authors could give their comparisons in training cost and inference time.\n3. Lack of extensive visualization comparisons with existing video generative models. The authors are encouraged to provide extensive qualitative comparisons in video generation task.\n"
    }
  },
  "SiT Dataset: Socially Interactive Pedestrian Trajectory Dataset for Social Navigation Robots": {},
  "Automatic Clipping: Differentially Private Deep Learning Made Easier and Stronger": {
    "review_16": {
      "summary": "The script proposed an automatic clipping methods for various DP algorithms. The problem is important because the performance of DP models are sensitive to the choice clipping threshold, yet there is no theorical guidance for tuning it. \n\n",
      "strengths": "The writing is clear and the paper is easy to follow. The idea is simple and effective.  Theory and experiments are provided to show the efficacy of the proposed automatic clipping method. I think this result is worth sharing with the DP community. \n\n\n\n",
      "weaknesses": "See below"
    },
    "review_17": {
      "summary": "This paper introduces 'automatic clipping' to replace the usual clipping operation in DP-SGD. The challenge of usual clipping is to choose a good threshold 'R' especially for deep learning models. The authors claim that automatic clipping maintains the same level of privacy and computational efficiency as existing DP optimizers such as DP-SGD, DP-Adam, and DP-LAMB, but without the need for any DP-specific hyperparameters. In fact, automatic clipping uses a kind of normalization to bound the sensitivity of individual gradient. Additionally, they provide a thorough convergence analysis for DP-SGD with automatic clipping in the non-convex setting, demonstrating that it can match the convergence rate of standard SGD under certain conditions. The authors validate their proposal by showing that automatic clipping either matches or surpasses the state-of-the-art performance on a variety of language and vision tasks.\n",
      "strengths": "1. **Simplicity**: The proposed method of automatic clipping significantly simplifies the process of DP training by eliminating the need for tuning DP-specific hyperparameters, thereby making DP as user-friendly as standard non-private training.\n\n2. **Performance**: The paper demonstrates that the automatic clipping method is either on par with, or better than, existing state-of-the-art techniques in a variety of tasks. This suggests that the method does not compromise performance for ease-of-use.\n\n3. **Rigorous Analysis**: The authors provide a detailed convergence analysis of automatic DP-SGD in a non-convex setting, showing that it can match the convergence rate of standard SGD under the symmetric gradient noise assumption of per-sample gradients.",
      "weaknesses": "**Misleading justification**: In figure 2, the paper present the dot products of $\\langle g_i, \\sum_i g_i \\rangle$. First it does not make sense if the dot product is properly normalized.  Moreover, the distribution of  $\\langle g_i, \\sum_i g_i \\rangle$ does not necessarily align with good performance. The illustration in Figure 2 is rather misleading.\n\n**Too strong Assumptions**: The proposed automatic clipping method's performance is based on a symmetric gradient noise assumption of the per-sample gradients. This assumption is too constrained and does not satisfy in practice.\n\n**Overclaim**: Section title 4.3 \"Automatic clipping is equally private and maximizes utility\" is somewhat overclaiming. It is not carefully argued why automatic clipping can \"maximize utility given a privacy budget.\n\n**Missing reference**: The paper missed a very related reference Yang et al. \"Normalized/clipped sgd with perturbation for differentially private non-convex optimization\" which also studied the DP-SGD with normalization. The paper should have a careful comparison with it."
    },
    "review_18": {
      "summary": "The paper discusses a way to control the sensitivity of DP-SGD by a method that does not require clipping.\n",
      "strengths": "- The motivation for the paper is clear\n- The authors make a compeling argument that suggest why their method should work\n- They have sound theoretical guarantees and experiments\n- The idea is simple yet fully developed\n- The impact this could have on private learning is huge\n- Experimental results are compeling\n",
      "weaknesses": "N/A"
    },
    "review_19": {
      "summary": "The author(s) proposed a new gradient clipping technique for differential private training algorithms. It was shown that the new clipping technique is more robust to hyper-parameters and can save the time for hyper-parameter tunning. Convergence of the proposed method under a gradient symmetric assumption is developed and experiments on both vision and language tasks are conducted to evaluate the proposed method.",
      "strengths": "- The paper is well-written and easy to follow in general.\n- The experiments are thorough and the algorithm seems to be useful on the practical side.",
      "weaknesses": "- The convergence analysis is based on a gradient symmetric assumption, therefore the utility bound in Theorem 4 is not really comparable to prior works. Please also see my quetions below. Overall, the theoretical contribution seems limited. However, given the strong empirical performance of the proposed method, this might be acceptable.\n- Some related work should be added [1,2,3].\n\n[1] Clip21: Error Feedback for Gradient Clipping, https://arxiv.org/abs/2305.18929\n\n[2] Improved Convergence of Differential Private SGD with Gradient Clipping, https://openreview.net/forum?id=FRLswckPXQ5\n\n[3] Normalized/Clipped SGD with Perturbation for Differentially Private Non-Convex Optimization, https://arxiv.org/pdf/2206.13033.pdf"
    },
    "review_20": {
      "summary": "The paper proposes a new alternative version of the DP-SGD algorithm where the gradients are normalized. This new method allows for a proof of convergence of the gradient when we add a small stability constant to the rescaling factor. This work eliminates the need for a costly 2D gridsearch between learning rate and clipping constant when training DP models. The authors then proceed to run experiments on to show their method equals or outperforms Abadi\u2019s DP-SGD algorithm.",
      "strengths": "The article eliminates a costly operation in DP training and provides strong evidence regarding the convergence of their method. The exhaustive list of datasets on which the method was tested testifies as to how only minimal changes to existing implementations are needed to apply this method.\n\n### Originality\n\nThe method is original.\n\n### Quality\n\nThe experiments presented are very complete and yield high quality results. Also, various supporting githubs and ressources are mentionned, making these results reproducible.\n\n### Clarity\n\nThe article is clear and well written.\n\n### Significance\n\nThe original motivation of the article is clear. ",
      "weaknesses": "## Sensitivity of the hyperparameter optimization\n\nThe results are not clear enough to indicate whether the proposed DP training method is superior to the rescaled DP-SGD method which impedes the significance of the article. If the authors of the paper manage to prove that their method is less expensive than the rescaled DP-SGD trick then the contribution is significant. The article lacks a conclusive experiment or statement proving that the sensitivity of the hyperparameter optimization process is easier with this method than with the re-scaled version of Abadi\u2019s DP-SGD. \n\nThe paper shows in its appendix H. that the training process of the AUTO-S method is relatively robust to the choice of $\\gamma$ on NLP tasks. However the paper doesn\u2019t compare the sensitivity of the **AUTO-S method** to the $\\gamma$ factor to the sensitivity of the **rescaled Abadi clipping** to the $R$ factor. In my opinion this is an important experiment to run, since it would justify using this method instead of the widely known rescaled DP-SGD version introduced in De et al. [15]. This could improve the impact and the significance of the contribution.\n\n### Actionable feedback\n\n> However, R remains and is expensive to tune (l223)\n\nThis sentence in particular is a bit of an overclaim in my opinion, see my comments below.\n\n**In your experiments you specifiy regarding the range of $\\gamma$**\n> \u201dunder which a wide range of 0.0001 < < 1 gives similar accuracy\u201d\n\nand\n>\u201cNote that the largest good is 1000 times bigger than the smallest good \u201d\n\n\n**In the comparison to related works on clipping**\nYou state \u201cSimilarly (De et al.) re-parametrizes the per-sample gradient clipping to mimic normalization. However R remains and is expensive to tune (see figure 8 of De et al.)\u201d. However this fig 8. shows that the re-parametrized per-sample gradient clipping seems to stay stable by a similar factor of approximately 1024 on the clipping norm, on a even harder dataset. Even though their experiment are not extensive enough to prove a superior efficiency of the rescaled DP-SGD method to yours, it is not conclusive enough for you to assume that your method is easier to tune.\n\nIf you add this missing experiment, or if you modify this claim, I would consider improving my rating."
    }
  },
  "A Theoretical Analysis of the Test Error of Finite-Rank Kernel Ridge Regression": {
    "review_16": {
      "summary": "This paper gives a refined analysis of the test error of kernel ridge regression with a finite kernel in the underparametrized regime, where the rank M of the kernel is smaller than the number of data points N. The analysis improves upon previous ones by providing tighter non-asymptotic upper and lower bounds, which remain tight even as the ridge parameter goes to zero. When the target function lies in the RKHS, the bounds show that estimation is consistent as N goes to infinity.",
      "strengths": "The paper is well written and clear. The authors state the contributions clearly and compare carefully with previous work. Establishing tight bounds for this basic learning setting is an important problem.",
      "weaknesses": "No major weaknesses, just some minor typos. I would also suggest that the authors include the explicit lower bounds in the main paper. Given that this a major focus of the paper it should not be required for the reader to go to the appendix. It would also be nice for the appendix to give a bit clearer account for the technical differences in the proof techniques of their bounds versus previous ones."
    },
    "review_17": {
      "summary": "The paper studies the kernel ridge regression under the non-asymptotic setting. The authors give the upper and lower bounds for bias and variance term, respectively. The authors argue the results improve upon those in Bach 2023.",
      "strengths": "The paper is well-structured. The authors give rigorous proofs, following by careful experiments.",
      "weaknesses": "1. The paper requires further improvement and polishing in writing. To name a few, line 57-58; line 107-110.\n\n2. The paper would benefit from a more consistent and standardized use of symbols and notations. For example, in line 90, it would be better to use L_2(\\rho) instead of L_\\rho^2. In lien 92, the notation of \\tilde{f}(\\textbf{X}) is not proper. In Definition 3.1, it would be better to include the decreasing order of eigenvalues in the statement rather than adding an additional remark 3.2. In line 200-207, notation K^{(\\infty)}\n\n3. As mentioned in Bach 2023, more refined bounds can be found in Rudi et al. 2015, Rudi and Rosasco 2017. However, the authors failed to mention them and other related results in the comparison. In the absence of such comparisons, it is hard to tell the novelty and improvements of the current submission.\n\n4. The dependency of \\lambda seems to be incorrect for the variance term.\n\n5. Given Corollary 4.3.1 in the submission, I cannot see significant improvements against those in Bach 2023. Also, the authors did not give a proper explanation for considering \\lambda goes to 0."
    },
    "review_18": {
      "summary": "This paper analyzes the test error of ridge regression with a finite rank kernel. The finite rank kernel appears in several practical settings such as transfer learning and random neural networks. The authors provide a new analysis in this setting using tools from random matrix theory. A detailed comparison to other generalization bounds is presented.",
      "strengths": "1. New generalization results in a practical and challenging setting.\n2. New analysis techniques.\n",
      "weaknesses": "1. The comparison to standard generalization results is not clear (Eq. (12)). Both bounds scale as $\\sqrt{\\frac{\\log(n)}{n}}$. It is not clear which one is tighter.\n2. The technical details of the proof are not given and the novelty of the analysis is not explained in detail. Instead, most of the paper is devoted to a discussion and experiments.\n3. There are some unclear technical issues (see questions below).\n"
    },
    "review_19": {
      "summary": "The paper highlights the inadequacy of existing statistical learning guarantees for general kernel regressors when applied to finite-rank kernels. The authors have addressed this issue by developing non-asymptotic upper and lower bounds for the test error of finite-rank kernel ridge regression. These new bounds are more precise than the previously established ones and are applicable for any regularization parameters. This research provides a more dependable framework for utilizing finite-rank kernels in machine learning applications.",
      "strengths": "1. The paper addresses an important gap in the current understanding of machine learning algorithms by developing more accurate and reliable bounds for finite-rank kernel ridge regression, which is frequently used in practical applications.\n\n2. The research provides a more precise and dependable framework for using finite-rank kernels in machine learning problems, which could result in better performance and more efficient algorithms.",
      "weaknesses": "1. The paper only considers under-parameterized regime.\n\n2. Low bound is a main argument of this paper, but all details are given in the Appendix.\n"
    },
    "review_20": {
      "summary": "The authors address the problem of kernel ridge regression with a finite rank kernel, in the under-paramatrized regime. They prove sharper upper bounds on the test error, compared to previous existing works.",
      "strengths": "The discussion is very well-written and easy to follow, with a number of illustrations being provided to set up the problem. Careful and detailed comparison to previous work is given, making it easy to understand the novelty of the present manuscript. Overall, the addressed problem is an important one, and I believe the derived bound will be of interest to the community. I have not checked the proof, and therefore give a low confidence score.",
      "weaknesses": "I do not have any major concern. A minor remark is that while all experimental details are provided in Appendix D, it would be good to add at least a short description of the target functions and regularization used in Fig.1 and 2 in the main text or in the corresponding captions."
    }
  },
  "Comparing Optimization Targets for Contrast-Consistent Search": {},
  "Fine-Tuning Language Models with Just Forward Passes": {
    "review_13": {
      "summary": "This work introduced a memory-efficient zeroth-order optimizer that can fine-tune large language models with the same memory footprint as inference, using only forward passes and gradient estimates. Comprehensive experiments across model types, scales, and tasks, showing that MeZO outperforms zero-shot, in-context learning, and linear probing, and achieves comparable performance to fine-tuning with backpropagation, while reducing memory cost by up to 12 times. Non-differentiable objectives that MeZO can optimize, such as accuracy or F1 score, which are usually not amenable to backpropagation. Theoretical insights that explain why MeZO can optimize LMs with billions of parameters, despite classical zeroth-order analyses suggesting otherwise.\n",
      "strengths": "1. It proposes a novel and memory-efficient method to fine-tune large language models without backpropagation, which can save up to 12x memory compared to standard methods.\n\n2. It demonstrates that the proposed method can achieve comparable or superior performance to fine-tuning with backpropagation across various tasks, models, and tuning techniques.\n\n3. It shows that the proposed method can optimize non-differentiable objectives, such as accuracy or F1 score, which are useful for many applications.\n\n4. It provides theoretical insights on why the proposed method can overcome the classical limitations of zeroth-order optimization and leverage the benefits of pre-training and task prompts.\n",
      "weaknesses": "1. While the experiments demonstrate good performance on the language understanding tasks, it is unknown whether the method is also applicable to the generation tasks.\n\n2. It relies on the assumption of low effective rank of the Hessian matrix, which may not hold for all loss functions. It would be great to have a discussion about the scope of application for the proposed method.\n"
    },
    "review_14": {
      "summary": "The paper proposes an enhanced memory efficient zero-order optimization method named MeZO. MeZO only requires the same memory as inference time and thus can enable model tuning for large LMs with limited memory budget. The authors demonstrate the efficacy of MeZO on multiple NLP benchmarks compared with linear probing, in context learning and fine-tuning in few/low shot learning regime. The authors also provided detailed theoretical proof for MeZO. ",
      "strengths": "0 - The authors targeted a not well understood domain (zero order optimization) and open up new opportunities for future works. In the era of LLMs, this method can enable many future work especially for those who don't have access to large-scale GPU clusters. \n1 - Comprehensive experiments and ablation studies on the proposed method.\n2 - Strong theoretical support on the proposed method.\n3 - Good writing and flow which makes the paper easy to follow and understand. \n4 - Well-articulated future work.",
      "weaknesses": "No major weaknesses. I left some comments in the questions section and hope  the authors can answer and address. "
    },
    "review_15": {
      "summary": "The paper present a new optimiser MeZo based on stochastic approximation using gradient perturbation. \nThis optimiser is very memory efficient as it only requires to perform 2 forward passes with different deltas/epsilons on the parameters and multiple gaussian samplings.   These algorithms allows \"finetunning\" large language models to specific tasks very efficiently ( up to 30B on a single A100) yielding between x4 to x12 memory reductions. Since the proposed algorithm is an optimizer it can be combined with other standard techniques such as LORA or prefix tunning. All this is applied in finetunning setups similar to ICL. ",
      "strengths": "The algorithm is a new application of well known stochastic gradient approximation but many times forgotten due to their slowness. \nIt is very surprising that this algorithm works, and the authors provide a theoretical justification of why this could be working in this case. \nThey also acknowledge that despite of what it might sound this approach only works in prompting fine-tunning scenario [Appendix B.2]. \n\nThe results are insight full , the baselines are fair and the theoretical analysis is correct to the best of my knowledge.\n\nThis can settle as an alternative to in context learning with prompting, by  fine-tuning with  a limited set of examples (512). \n\nThe proposed technique seems to work on the benchmarks used and seems to surpass other techniques such as Zero-shot, LP, ICL, and approaches very close performance to fine-tunning. This can be set a cheap alternative to ICL for some tasks.\n\n",
      "weaknesses": "While there was a huge and titanic effort in the paper there were many questions that steam from the technique.\nThe first area which has not been explored too much and given as true is the need of having a prompt to apply MeZO.\nThis key ingredient is not well studied but rather given on some preliminary experiments, e.g. Table 5. \nWhy is MeZO not working w/o prompts, even some very simple prompts ? \n\nThe need of the prompt also raises the question of how this is related to ICL , as there are some works that suggest ICL maybe doing some alike to fine-tuning or back-propagation though the attention weights. Is this combination of prompting and MeZo that is guiding the forward propagations ? is there a mixed cooperation between the prompt and the stochastic technique ? How much of the prompt is needed ?\n\nAnother question would be how the different techniques behave as a function of the number of examples k. It would have been nice to see a plot for some models at least between ICL , MeZo, and possible ft on the selected tasks. Why have authors stopped at 512 ? why only 16 or 512 ? there are some dataset that contain more training examples. Why didn't they compare fine-tuning and MeZo in other setups with larger examples ?\n\nThere is the relationship between the task itself and the optimiser. It is not clear to me, in which tasks this will work properly. I suspect given the prompting above that this might only work on low-perplexity tasks or task in which prompting or ICL can generate good results and not in other more complex tasks. \n\nClearly this is maybe too much to address in the paper, but all aspects above point toward the little understanding the reader is left with on under which conditions this technique can be applied. The future work seems to already assume the MeZO algorithm is working and proved, but there is just an hypotheses and a very low link between the experiment conditions and the theory. The link is stablished as \"We attribute these phenomena to the Hessian of the loss exhibiting  small local effective rank.\" . It would have been nice to strengthen this connection with some experiments or computation. Could this explain when or how this algorithm is applied ? if we remove the prompt does this increases the H effective rank ? would other tasks exhibit larger ranks ?  how can we reduce it for each of the tasks ?\n\n\nPlease, I would kindly ask the authors to read above questions and discussion as a signal of the interest the paper brought to the research field and not as criticism on their very interesting work.\n"
    },
    "review_16": {
      "summary": "This paper proposes a new zeroth order optimizer, MeZO, for LM training.  This is proposed as an improvement to ZO-SGD.  The advantage of this approach is a 12x reduction in the amount of memory required for training compared to backpropagation.  This enables the training of much larger models.\n\nThe effectiveness of MeZO is shown across a range of benchmarks and model sizes.  The results compare favorably to linear probing and in context learning.",
      "strengths": "The MeZO technique stands to unlock substantial capability for LM training.  This enables training of much larger networks.  The compatibility with LoRA, prefix tuning are important use cases for many LM users.  There is an ability for optimizing non-differentiable objectives which is compelling, and could be expanded in the future.\n\nThe empirical behavior are coupled with a section on theory which effectively describes the both the expected behavior, but elaborates on the expected slow convergence by expanding the theoretical analysis to address low effective rank networks.",
      "weaknesses": "While the analysis refers to the convergence rate of MeZO, there is a very brief treatment of convergence behavior in the paper (Appendix E.2) It might be helpful for this to be expanded and possibly compared to backpropagation, especially in the context of the presentation of Section 4 Theory."
    },
    "review_17": {
      "summary": "Fine-tuning with backpropogation becomes infeasible for very large language models because it uses too much memory. While zeroth-order optimization uses far less memory and could in principle fine-tune the model with just forward passes, past theory suggested that the learning rate must scale down with the number of parameters, making convergence prohibitively slow. However, this paper finds that zeroth-order optimization actually performs quite well and converges quickly even on very large language models. They provide theory to explain this fast convergence, where they show that under an assumption they call \"low effective rank,\" the learning rate scales down with the rank rather than the number of parameters. They also provide a memory-efficient implementation of zeroth-order optimization that they call MeZO, along with memory efficient zeroth-order versions of SGD with momentum and Adam. In experiments, the method performs similarly to backpropogation with 1/12 the memory usage, while outperforming in-context learning and linear probing.",
      "strengths": "(1) The paper is well-written.\n\n(2) The method is simple and easy to understand.\n\n(3) The theory provides useful insights into why zeroth-order optimization works for fine-tuning large pre-trained models.\n\n(4) The experimental results are strong, and the appendix contains thorough ablations.\n\n(5) The idea that zeroth-order optimization works well for fine-tuning LMs seems practically useful and addresses a pressing need in the community for memory-efficient methods.",
      "weaknesses": "The paper seems strong overall, and I support its acceptance regardless of whether the suggested experiments below are run or not during the rebuttal period.\n\n(1) From what I understand, the paper does not verify the low effective rank assumption empirically, nor is it verified in the papers cited (which either study the effective rank / Hessian spectra in non-LLMs, or study the LLMs but not the effective rank and instead study the intrinsic dimensionality of fine-tuning). Therefore, to justify the assumption, it seems useful to study the Hessian spectra of the downstream fine-tuning loss in LLMs, at whatever size is feasible.\n\n(2) Related to (1), to verify the theory and confirm that the effective rank is indeed the quantity that determines convergence rates, it seems useful to run simulated experiments where one constructs a synthetic model + data and varies the effective rank, and examines whether the convergence rate or gradient norm scales with the effective rank as predicted in the theory."
    }
  },
  "PIXIU: A Comprehensive Benchmark, Instruction Dataset and Large Language Model for Finance": {},
  "FLAb: Benchmarking deep learning methods for antibody fitness prediction": {},
  "Sparse Parameterization for Epitomic Dataset Distillation": {
    "review_11": {
      "summary": "This paper proposes a new parameterization for dataset distillation. The new parameterization considers image patches, use sparse matrix and recurrent feature net to generate synthetic images. The total parameters follows storage constraint. The experimental results show improvement over previous methods.",
      "strengths": "+ This paper proposes a new parameterization for synthetic images in Dataset Distillation. \n+ The proposed method works well across various benchmarks, including imagenet, cifar and cross arch generalization\n+ This paper is quite interesting in proceeding the research on Dataset Distillation parameters. Spatial redundancies are quite dominant in standard parameterization, and this method can certainly help with alleviating that.",
      "weaknesses": "- The paper claims on reducing spatial redundancies. I wonder how the authors compare their method with [19]. Does that also consider reducing spatial redundancies? \n- The paper provides better empirical performance, but the research messages are not quite surprising.\n- Have the authors considered using algorithms similar to [a] for sparsification?\n\n[a] Parameter-Efficient Transfer Learning with Diff Pruning"
    },
    "review_12": {
      "summary": "This work proposes a new memory-saving method of dataset distillation by distilling the dataset into a set of Spatial-Agnostic Epitomic Tokens which are indexed by Sparse Coding Matrices and decoded into images by a Feature-Recurrent Network. This method is plug-and-play compatible with existing distillation methods, allowing them to achieve more efficient storage. State-of-the-art results are shown for many datasets and problem settings. ",
      "strengths": "The overall presentation of the paper is very nice. The figures and equations very clearly explain to the reader the main ideas. The colorfully annotated Eq 7 especially makes the storage budget easy for the reader to digest. \n\nThe SPEED method itself is quite interesting, and algorithm 1 very clearly explains the process.\n\nThe many ablation studies and side-experiments further explain the effectiveness of the method.",
      "weaknesses": "The biggest issues I have with this paper are the presentation of tables 1 and 2.\n\nThis is not an issue specific to this paper, but methods that do re-paramaterization as a means of memory saving should not be directly compared to methods that only propose a matching algorithm.\n\nIt should be made extremely clear that IDC, HaBa, RTP and SPEED are solving an inherently different problem than the baseline methods.\n\nSynthetic set size should also not be given in \"IPC\" but in the number of learnable parameters, since the given IPC simply isn't true anymore. For example, instead of IPC=10, you could have #Params <= 30,720 (10x3x32x32)"
    },
    "review_13": {
      "summary": "The paper proposes a new framework(SPEED) to perform dataset distillation.\n\nThe new framework is composed of 3 parts:\n1. Spatial-Agnostic Epitomic Tokens (SAETs)\n2. Sparse Coding Matrices (SCMs)\n3.  A Feature-Recurrent Network (FReeNet)\n\nThe paper also employees multi-head attention to ensure the diversity of distilled images, thus achieving new SOTA. \nThe new framework SPEED is also claimed to work with multiple dataset matching methods and enhance their performances.\n\nThrough various experiments, the method shows strong performances on CIFAR-10/100 and TinyImageNet. Similar results are also observed on ImageNet subsets.\n\n### I have read the author's response and my concerns are addressed by seeing more experiment results.",
      "strengths": "## originality\nSPEED is claimed to be the first paper studying the spatial redundancy in the field of dataset distillation/condensation\n\nSPEED applies a few methods such as the concepts from ViT, dictionary learning and sparse coding.\n\n## quality\nThe method works very well on dataset with higher resolution.\n\n## clarity\nThe paper is well written and easy to follow\n\nThe framework of the method and learned images are visualized which makes it easy to understand.\n\n## significance\nThe proposed methods achieve SOTA performances under most settings.\n\nThe paper novelly proposes to distill images at image patch level which shows a new way for dataset distillation.\n\nThe proposed method is compatible with multiple matching objectives.",
      "weaknesses": "- Incomplete evaluation results in table 2. IPC 1/10/50 are used for CIFAR-10/100, but only IPC 1 for TinyImageNet is used. Why that is the case? From [1], even the trajectory matching method this paper adopts has reported the results on TinyImageNet with IPC 1/10/50.\n\n- In table 1, for the parameterization methods including SPEED, can the author include how many synthetic images are generated for evaluations, e.g. 11 for IPC 1 so that we know if the performance gain is due to increased number of images or increased quality of generated images.\n\n\n\n\n\n\n\n[1] Dataset Distillation by Matching Training Trajectories"
    },
    "review_14": {
      "summary": "This paper introduces the insight of sparse coding into dataset distillation and proposes a sound method. This work can efficiently generate syn data by adopting the multi-head SCMs as the shared source of the syn images and using a recurrent model to generate the syn patches. It can cooperate with various previous matching methods. In extensive experiments, the proposed method shows auspicious results and superiority.",
      "strengths": "+ Importing the sparse coding into the syn data is interesting and effective from the experiments. The analysis of the problem of overlooking the syn data sparsity itself is insightful and naturally leads to the proposed method.\n\n+ Good writing, easy to follow.\n\n+ Extensive experiments, ablations, and visualizations.\n\n+ Insightful and valuable discussions and analyses, especially the syn data property, clear formulation, storage analysis, and trade-off between quality and quantity.\n\n+ Fig III in the appendix clearly showcases the effects of the proposed method.",
      "weaknesses": "- More discussions about the relations between dataset distillation, sparse coding, and corset selection.\n\n- Though the recurrent model is sound according to the cost discussion. I still wonder about the performance of adopting a heavier model following the same idea of the proposed method.\n\n- Fig. 4: should be more self-contained, please explain the comparison and differences/similarities."
    }
  },
  "Truly Scale-Equivariant Deep Nets with Fourier Layers": {
    "review_15": {
      "summary": "This paper proposes a formulation of scale equivariant networks, which takes anti-aliasing into account. This is in contrast to previous works, in which the issue of aliasing is often disregarded. The proposed method achieves competitive classification accuracy on MNIST-scale and STL-10, while maintaining zero equivariance error.",
      "strengths": "This work addresses a problem that has been thus far disregarded when talking about scale equivariance: the aliasing issue. The proposed methods and solutions are sound and are empirically evaluated with very good results.",
      "weaknesses": "* My main concern is that, to the best of my understanding, the proposed framework does not consider inter-scale relationships. Instead, it considers multiple scales in parallel which are processed independently of one another. This brings into question whether this work should be defined as a work in group equivariance or rather a work on models aiming to be resolution-agnostic, i.e., able to work regardless of the resolution of the input, e.g., FlexConv [1], S4ND[2], \\inf-Diffusion [3].\n\n* This method relies heavily on using the Fourier and inverse Fourier transform for each of its layers. In addition, it is well-known that performing convolutions on the spatial domain is faster and requires less memory than performing them in the Fourier domain when convolutional kernels are small O(Nk) vs O(NlogN). However, the proposed method actually relies on small convolutional kernels. These two factors call into question the computational and memory complexity of the proposed method. I believe that this is the main practical limitation of the method, and I would like the authors to discuss this in the paper \u2013in case it is indeed an issue\u2013.\n\n* Connecting to the previous point, I believe that having a section in which the limitations of the work are clearly stated and discussed is vital."
    },
    "review_16": {
      "summary": "The submitted paper explores the problem of scale-equivariant neural networks.\nTo get the desired property by design, the authors propose new building blocks: a spatially local Fourier layer, a novel scale-equivariant pooling layer, and scale-equivariant way of applying activation functions.\nAll the introduced modifications rely heavily on the Fourier transform of the input feature map. \nAdditionally, a new loss function is proposed which aims to promote the natural consistency of predictions made at different scales. \n\nThe effect of the new approach is demonstrated on the problem of image classification on two datasets: MNIST-scale and STL-10-scale. ",
      "strengths": "I find the proposed modifications well explained and motivated. \nTo the best of my knowledge, the presented approach is original enough, and it may be interesting for the broad community. \nQuality of the results seems very satisfactory and I hope they can serve as a reliable baseline for further research on this topic. \n",
      "weaknesses": "1. While the proposed results seem strong enough for the task of object recognition, the authors have not reported the computational or memory overhead introduced by their approach. Without this knowledge, it is hard to understand if their method can be directly scaled to more high-resolution data without any significant changes.\n\n1. I admit that most prior papers also mostly report their performance for image classification, it is quite clear that the property of scale equivariance may be even more desirable for the dense prediction tasks, e.g. semantic segmentation. Are there any barriers to applying the same method to the common segmentation architectures?\n\n1. Regarding the low performance of baseline models on STL-10-scale data (line 274), have the authors tried other padding schemes than zero-padding, e.g. so-called reflect/replicate/circular strategies (see the [link](https://pytorch.org/docs/stable/generated/torch.nn.functional.pad.html))? I find it suspicious that, e.g. the reported DISCO performance of 91.93% accuracy on STL-10 [1] drops to 47.68% on STL-10-scale (Tab. 4) in the provided evaluation results. I think, more discussion is needed on that matter.\n\n[1] Sosnovik et al. DISCO: accurate Discrete Scale Convolutions. In BMVC, 2021."
    },
    "review_17": {
      "summary": "This paper introduces Truly Scale-Equivariant Deep Nets with Fourier Layers, which directly formulate down-scaling in the discrete domain to address the anti-aliasing problem. The proposed methods are validated on the MNIST-scale and STL-10 datasets, demonstrating good classification performance while preserving zero equivariance error.",
      "strengths": "1. This paper addresses how to learn scale-equivariant representations in deep neural networks, overcoming the limitation of previous methods that did not account for anti-aliasing.\n2. The paper proposes a method that can consider the down-scaling operation directly in the discrete domain with anti-aliasing and achieves good performance.\n",
      "weaknesses": "1. This paper does not provide sufficient experiments to demonstrate the generality of the proposed method. It only tests on two small datasets and a single high-level vision task (image classification), which limits the impact of the proposed method. This paper should also evaluate the method on more larger-scale datasets, more high-level vision tasks (e.g., image segmentation), and more CNN architectures.\n2. Why does the baseline use zero-padding to the original resolution as the input, instead of resizing to the original resolution? Does this affect baseline's representation learning and performance?\n3. The paper lacks an analysis of the computational cost of the proposed method and other compared baseline during training and inference.\n"
    },
    "review_18": {
      "summary": "The authors of this study introduce architectural modifications to the conventional CNN framework by incorporating Fourier layers. Their aim is to achieve scale-equivariance, addressing the anti-aliasing problem that occurs when images are downscaled in the continuous domain. To tackle this issue, the authors propose a discrete domain downsampling formulation and propose corresponding changes to the network architecture. Through experiments on a few datasets, the authors demonstrate that their method achieves zero scale-equivariance error.",
      "strengths": "- The paper is well-written, precise, and easy to follow. In particular, their motivation is stated clearly, they introduce the preliminaries succinctly, and the approach stated is coherent and structured.\n\n- To the best of my knowledge the proposed method is novel and the claims that the module modifications achieve scale-equivariance is correct.\n\n- I am unaware of any method but this that solves the chosen problem completely, i.e achieves zero scale-equivariance error when downsampling is done via anti-aliasing and subsampling ('ideal' subsampling). The results are also competent when they do 'non-ideal' subsampling when compared to standard baselines. \n\n- The motivation that images of higher-scale should achieve better classification accuracy than the ones of lower scales is reasonable and the corresponding proposed constraint is sound.\n\nOverall I think the ideas presented in this paper maybe of interest to the broad ML community.",
      "weaknesses": "- Due to the complexity involved in the several DFT/Inverse-DFT related operations, the baselines they compare to which involve kernel resizing are certainly not as costly as the proposed method. I suggest the authors to discuss how their method compares to the baselines in terms of computational/time complexity.\n\n- For the same aforementioned reasons, I suspect that the method might not scale to high dimensional datasets like ImageNet or standard architectures like ResNet50. This I think would be a fundamental limitation, therefore I encourage the authors to discuss the challenges in scaling their method to the mentioned settings. I would be interested to know which modules will be particularly harder to scale.\n\n- Some minor text changes:\n\n     - Line 119, Claim 1: 'even lower in frequencies' needs to be 'even in lower frequencies.'\n\n     - Line 138, Claim 2: 'equivaraint' needs to be 'equivariant.'\n\n- Figure 1, Figtitle: \u201cOn the other hand, downsampling the high-res feature is guaranteed to achieve the same low-resolution feature.\u201d -> Unclear formulation. It is not clear how the downsampled version of the features was achieved. The text suggests that it has merely been downsampled from the high-resolution version, but then it is not clear which role the new proposed architecture plays. Please rephrase for clarity.\n\n\n- Line 32: \u201cSpecifically, these works are derived using a continuous domain down-scaling operation, i.e., there is no need to consider anti-aliasing. However, when performing a down-scaling, the Nyquist theorem [21, 27] tells us that an anti-alias filter is necessary to avoid high-frequency content to alias into lower frequencies. (...) To address this gap, from prior work, we consider the down-scaling operation directly in the discrete domain taking into account of the anti-aliasing.\u201d Several grammar errors and the paragraph is unclear to me. In the first sentence, the authors write that previous works \u201cdo not need to consider anti-aliasing\u201d. But then, they aim to bridge the gap and take anti-aliasing into account. Please fix the formulation as it is currently very unclear whether previous works had an issue with anti-aliasing or not.\n\n\n- Line 103: The notation for the definition of g is unclear. It is unclear what is meant by that x is \\in {R^1, R^2 \u2026 R^N}, which makes it hard to understand the following definitions. The notation of {..} suggests that x can be from R^1, but then how is the Fourier transform of one number even defined? The Fourier transform is needed for D_R(x). I am confused by this definition, please rewrite for clarity. \n\n\n- It would be helpful if the authors would state that small x refers to the spatial domain and big X always refers to the Fourier domain. While this is the common practice, it would still be nice to state it explicitly.\n\n\n- I don\u2019t understand Fig. 2b. Are the yellow regions ones and the gray ones zero?\n\n\n- Line 142: \u201cElement-wise non-linearities, e.g., ReLU, in the spatial domain are generally not scale-equivariant under the ideal downsampling operation DR.\u201d The reasoning of this claim is not obvious to me, please explain / give some intuition.\n\n\n"
    }
  }
}