{
  "Inner Classifier-Free Guidance and Its Taylor Expansion for Diffusion Models": [
    {
      "summary": "This work presents a new perspective on classifier-free guidance (CFG) for diffusion models imposing specific assumptions on the space of condition. Assuming that the condition space has a cone structure, the previous CFG can be seen as the first-order Taylor expansion of the proposed ICFG, and this work further presents a second-order ICFG that improves the Stable Diffusion model.",
      "strengths": "- The observation that there exists a mismatch between the transition distribution of the original forward process and the conditional forward process is new to the best of my knowledge. \n\n- The idea that the mismatch is alleviated when assuming that the condition space has a cone structure is interesting.",
      "weaknesses": "- The contribution of the proposed ICFG is not clear: Although the authors state that the second-order ICFG introduces new valuable information, it is not clear which additional information it provides, and further the experimental results show a marginal improvement over previous CFG, e.g., FID improvement from 15.42 (CFG) to 15.22 (ICFG) and CLIP Score from 26.45 (CFG) to 26.86 (ICFG), on only single dataset (MS-COCO). Especially, ICFG does not seem to provide an improved balance between FID and CLIP Score. What is the main reason we should use ICFG instead of CFG?\n\n- As the main motivation of this work is the mismatch between the transition kernels (in Theorem 3.1), this should be further analyzed, for example, how much difference in these kernels and how much it affects the generation quality. The proposed method should be evaluated after these validations.\n\n- The assumptions (Assumptions 3.1 and 4.1) made to achieve the proposed method do not seem realistic and were not verified in the experiments."
    },
    {
      "summary": "This paper generalizes CFG for diffusion model guidance by adapting the guidance strength according to the relevance between the condition and a given sample. Conditions more relevant to a sample require weaker guidance strength.",
      "strengths": "The paper identifies an issue with the deviating SDE when guidance is added and proposes a solution with clearly defined assumptions. The paper strikes a good balance between theoretical analysis and empirical results. The theories are relevant to the technique and justify the design choice. Extensive ablation studies on hyperparameters of the method yield insight to the adoption of the technique in practice.",
      "weaknesses": "**Cone assumption**\n\nIt seems like the key point of ICFG working is for the conditional space to be a cone. Is there any method to check whether a conditional space is a cone beforehand for practioners to decide whether ICFG should be adopted? Are there any metrics for characterizing how cone-shaped a conditional space is?\n\n**Algorithm 3 relaxing 2nd order term**\n\nThe relaxation of the 2nd order term is concerning. The argument for ICFG with 2nd order Tayler Expansion working better than typical CFG is the additional information provided by the 2nd order term. However, if an additional hyperparmeter is introduced and optimized over, does this argument still hold? Does the performance gain of ICFG truly come from the 2nd order information or is it the mere additional of another tuning knob $v$?\n\n**3 forward passes for naive 2nd order ICFG implementation**\n\nThe last paragraph of the discussion section mentions one major drawback of 2nd-order ICFG, which would require 3 forward passes to estimate the 2nd order term. The authors mention a solution of \"reusing the points for the pre-order term\". Elaboration on how exactly this can be done is crucial for actual adoption of this technique. Paying a computation penalty of 3 forward passes is definitely not feasible. The authors should also compare theoretically and/or empirically how this approximation would affect the efficacy of ICFG."
    },
    {
      "summary": "This paper presents a generalized version of classifier-free guidance (CFG), i.e., inner classifier-free guidance. By exploiting the continuity of the generation condition, the author propose an interesting taylor expansion formulation to interpret CFG, where the classic CFG is viewed as the first order case of the proposed formulation. Given such novel formulation, the author proposed higher order version of CFG to achieve better image generation results.",
      "strengths": "1. The proposed formulation is novel and insightful.\n2. The paper is easy-to-follow.\n3. The hyper-parameters introduced by the method is well-studied.",
      "weaknesses": "1) In the reviewer's viewpoint, the major weakness of the current submission is that the empirical validation is not sufficient. More qualitative results should be provided to justify the effectiveness of the method. One example is that it would be better if the provided qualitative samples could corroborate with the numerical experimental results. Another example is that the author could provided some examples showing that ICFG can resolve some of the well-known failure cases of Stable Diffusion.\n\n2) In addition, it would be good to show the effectiveness of ICFG on other fine-tuned variation of Stable Diffusion such as anything-V4 (https://huggingface.co/xyn-ai/anything-v4.0), etc."
    },
    {
      "summary": "This paper introduces an enhancement of classifier-free guidance (CFG) for diffusion models, called inner classifier-free guidance (ICFG). The paper claims that CFG can be extended to ICFG when the condition is continuous, leading to further improvements. They provide a theoretical analysis based on the condition space assumption and Taylor expansion. They present the experimental results comparing CFG and ICFG.",
      "strengths": "The idea of combining the guidance strength and condition is interesting because adjusting guidance strength is the effect of a trade-off between image fidelity and condition information.",
      "weaknesses": "1. Concerns about the theoretical analysis in Section 3\n* In the proof of Theorem 3.1, there is doubt about the validity of the third equality in Eq. 16. It seems that it may come from Eq. 15, which might change the equality to the proportion.\n* Additionally, the relationship between the last two terms in Eq. 16 is the proportional relationship, but I'm not sure why these two terms are equal after the logarithm, as shown in Eq. 17.\n* In the proof of Theorem 3.1, it would be helpful to provide a detailed explanation of why there is a contradiction unless $w=0$ or $q(x_0|x_t)$ is a Dirac delta distribution.\n* Theorem 3.1 claims necessary and sufficient conditions, but the proof only demonstrates one direction.\n* In the paper, all cases of the enhanced intermediate distribution are denoted $\\bar{q}$. (In addition, $\\beta(x_t)$ in Eq. (12) is not defined.) This is confusing, especially for the definition $\\bar{q}(x_t|c,\\beta):=\\bar{q}(x_t|\\beta c)$.\n* In the proof of Corollary 3.1.1, it is unclear whether the first proportion holds. It seems that $q$ is modified by $\\bar{q}$. Detailed derivation and explanation would be helpful.\n\n2. Not significant experimental results\n* The quantitative results suggest that the performance gain is marginal, and this may require very careful hyperparameter tuning. It would be beneficial to include results from other datasets to assess the tuning problem.\n* The implementation of ICFG in Algorithms 2 and 3 requires three network evaluations for each timestep ($\\epsilon(z_i), \\epsilon(z_i,c), \\epsilon(z_i,mc)$), which is 1.5 times more network evaluations than CFG. Consequently, ICFG has 1.5 times higher sampling cost compared to CFG."
    }
  ],
  "Butterfly Effects of SGD Noise: Error Amplification in Behavior Cloning and Autoregression": [
    {
      "summary": "The paper examines a known problem in behavior cloning. This problem occurs during the training of such agents, where the validation criterion (return of an episode under long horizon rollouts) has large variance at all training steps, while the training surrogate loss has small, making it hard to perform model selection.\n\nThe authors perform a theoretical and empirical study of the origins of this problem. They argue that the observed variance is due to training instability, instead of insufficient dataset size, and they name the phenomenon as *Gradient Variance Amplification (GVA)*. They suggest that alternative training algorithms might not have the same issue. For this reason, they propose a very simple fix, which is to track the exponential moving average (EMA) of the parameter iterates of the SGD trained model. They demonstrate the effectiveness of the approach by performing experiments under various environments and by ablating design choices of the proposed EMA intervention.\n\nIn addition, they argue that GVA generally exists whenever agents are expected to operate by conditioning themselves in their past output (or effects of them). This definition aligns with conditional language modelling, and they argue with experiments that language generation quality in LLMs is similarly affected and that EMA can also help mitigate the problem there.",
      "strengths": "* The paper is well-organized, well-written and well-argumented. It introduces and motivates the problem sufficiently and discussed related work in depth.\n\n * They pose a single clear question at the heart of the problem: Do we just need more data? Or is there in the training algorithm that amplifies variance for the validation criterion? Their ablations convince that the second is the case (section 3.1/figure 2), and they provide with theoretical insight that the return in horizon H can be exponentially large for some suboptimal policy in a smooth neighborhood around the expert, even though for all of those suboptimal policies in the neighborhood the surrogate training loss (of behavioral cloning) is small.\n\n * They introduce various decision choices around implementing EMA as a candidate solution, and they ablate many of them, demonstrating that the problem is effectively mitigated in many RL environments.\n\n * They make connections to the relevance of the problem to other topics in ML, such as autoregressive language modelling.",
      "weaknesses": "1. Ablations regarding the design choice of $\\gamma_t$ scheduling are missing. In the paper, a polynomial decay is used and ablated, but other schedules have also been used with EMA (like cosine). It would be nice to understand a bit better why this decay is important and how to design one which is tailored at the problem at hand.\n2. Middle of Figure 4 misses y-axis values, which is important in order to know at which scale are we seeing the zoom at.\n3. In the context of autoregressive language generation: Validation perplexity perhaps does not show the existence of GVA problem here in the most clear way. Ideally, some equivalent to a metric of generation quality of horizon H autoregressive rollouts should have been used instead."
    },
    {
      "summary": "The paper examines how noise in gradients affects systems with feedback loops. It argues that SGD error accumulates over longer horizons, leading to Gradient Variance Amplification (GVA), explaining sharp reward oscillations. GVA is shown to be the main cause of these oscillations, surpassing the influence of statistical and architectural factors. Empirical evidence shows that the empirical moving average (MVA) mitigates these oscillations, ensuring stability. Additionally, the paper revisits the theory of stochastic optimization for convex functions, assessing its explanatory power for empirical observations of EMA and various step size schedules.",
      "strengths": "a) The paper conducts meticulous experiments revealing that the variance in stochastic gradients is the true source of instability. It introduces exponential moving average as an effective solution to mitigate this issue. Additionally, this phenomenon is also demonstrated for other tasks with feedback loop, i.e., auto regressive processes for language generation. \nb) The paper is well-written and the main message is clearly presented.",
      "weaknesses": "a) The main problem of the paper in my opinion is the explanation provided for the benefits of EMA.  Proposition 3.1 says that land scape is very intricate and for every $\\delta$ there is a separation between $J$ and the behaviour cloning loss. However, the *cliff*-type loss framework used to study this does not capture this behaviour, as it is small in a neighbourhood of radius $\\epsilon $ and is very large outside. In my opinion, this framework it too tailor-made to study the benefits of EMA and does not reveal the real reason behind its working mechanism."
    },
    {
      "summary": "The paper proposes that the training instability in the policy network is due to the gradient variance amplication during training",
      "strengths": "The idea is novel and the problem being dealt with is sufficiently important, namely, during training in reinforcement learning, the model does experience strong instabilities, and good methods to mitigate them would be of great importance",
      "weaknesses": "I am quite confused by the main claim of the work. \n\nThe main claim is that the gradient variance is responsible for the instability and that it amplifies throughout training, but no numerical result in the paper really plots the gradient variance, and, of course, not a single experiment shows that this variance is amplified. It should not be difficult to plot the variance of the gradient at all, and the absence of such evidence makes it impossible for me to recommend acceptance."
    }
  ],
  "G$^2$N$^2$ : Weisfeiler and Lehman go grammatical": [
    {
      "summary": "This paper investigates the expressive power of 3-WL from the aspect of formal language. The authors show that 3-WL is equivalent to a context-free grammar (CFG), and propose a reduced CFG that preserves the same expressiveness. Based on the reduced CFG, the authors develop a new WL algorithm and GNN model that match the expressiveness of 3-WL. The new GNN model achieves competitive performance and efficiency on downstream tasks.",
      "strengths": "There are some positive points of this paper. \n* The paper is well-written and easy to follow.\n* The paper exploits the formal language equivalence to investigate the GNN model and design a new GNN model, which I think is a promising direction for future research.",
      "weaknesses": "I have some concerns about the paper as follows:\n* I am not convinced by the novelty and the contribution of the paper, as the CFG $G_\\mathcal{L_3}$\u200b\u200b seems to be a straightforward derivation of the MATLANG.\n* The validation and discussion of the reduced CFG and the corresponding GNN may be insufficient, both empirically and theoretically. I have some questions for the authors below."
    },
    {
      "summary": "This paper presents a framework to convert context-free rules over an algebraic matrix language into a GNN architecture. Using this framework, they produce a WL-3 GNN as follows: (1) they write down a set of context-free rules producing a language that is just as expressive as 3-WL, (2) they reduce this set of rules into a smaller set of rules, and (3) they translate these rules directly into a GNN architecture. The resulting architecture performs competitively in practice, outperforming various existing GNNs on a variety of tasks.",
      "strengths": "(1) While this is not my area, the contribution of the paper seems strong in that it presents a framework for designing GNN architectures that implement a given CFG.\n\n(2) The experiments seem strong, and the proposed GNN is both provably expressive and performs competitively compared to existing architectures.\n\n(3) The paper is well-written, clear, and well-organized.",
      "weaknesses": "Some minor weaknesses are discussed in the questions section."
    },
    {
      "summary": "The paper proposes a new architecture for GNNs that captures precisely the expressive power of 3WL. This architecture is based on a grammatical representation of a language over graphs that has the same expressive power as 3WL. The idea is that this new architecture permits a more efficient implementation than the 3WL-based GNNs, which are known not to scale well in practical scenarios.",
      "strengths": "- The paper is very polished and easy to follow\n- The topic is timely and the problem practically relevant\n- Experiments confirm the suitability if the approach",
      "weaknesses": "There is only one criticism I make to the paper and it is the lack of search for a principled explanation of why the GNNs based on MATLANG are more efficient than the ones based on 3WL."
    }
  ],
  "Light Schr\u00f6dinger Bridge": [
    {
      "summary": "Summary\n1) The authors propose a novel light solver for continuous SB with the Wiener prior, i.e., EOT with the quadratic transport cost. The solver has a straightforward non-minimax learning objective and uses the Gaussian mixture parameterization for the EOT/SB, which avoids the time-consuming max-min optimization, simulation of the full process trajectories, iterative learning, and MCMC techniques that are in use in existing continuous solvers.\n2) The authors show that their novel light solver provably satisfies the universal approximation property for EOT/SB between the distributions supported on compact sets.\n3) The authors demonstrate the performance of the light solver in a series of synthetic and real-data experiments, including the ones with the real biological data considered in related works.",
      "strengths": "Strengths:\n- The authors provide a light solver for SB that does not rely on neural network parametrization.\n- This work provides a universal approximation for the solver which seems to be rather non-trivial. But to be honest, I'm not familiar with the proof so I'm not sure of the technical depth.\n- A minor thing but I really appreciate the authors giving a very clear description of the limitations of the solver.\n\nFollowing the rebuttal, the authors also provided statistical guarantees for the method.",
      "weaknesses": "Weaknesses:\nThis work does not have a finite-time nor finite-sample convergence guarantee. I believe that with additional assumptions one can obtain convergence guarantees as in [1]. Furthermore, there have been multiple works on the sampling complexity of quadratic cost EOT, and given the equivalence between EOT and SB, I believe that obtaining guarantees should be feasible.\n\nI also think that the experiments are not comprehensive enough. While the results are nice, I'm wondering what is the runtime performance of the light solver against other solvers. I cannot vouch for the method if I don't know how the method would improve in terms of the quality of the result and the computational cost.\n\nThe authors did mention that the work relies on Gaussian mixture parameterization and the entropic cost. It is indeed a limitation but personally, I think it is fine. Nevertheless, I will still raise this as a potential weakness.\n\nAll and all, I like the paper but it does not convince me enough to vouch for its acceptance yet.\n\nMinor comments:\nI think the literature on EOT is not sufficient. The authors should include gradient-based EOT solvers such as [2] (both of these methods achieved the optimal O(n^2/eps) complexity, which is stronger than that of APDAGD and seems to have good performance) and a gradient-based entropic UOT solver [3].\n\nReferences:\n[1] Chen, Y., Deng, W., Fang, S., Li, F., Yang, N. T., Zhang, Y., \u2026 Nevmyvaka, Y. (2023). Provably Convergent Schr\u00f6dinger Bridge with Applications to Probabilistic Time Series Imputation. Retrieved from http://arxiv.org/abs/2305.07247\n[2] Xie, Y., Luo, Y., & Huo, X. (2023). An Accelerated Stochastic Algorithm for Solving the Optimal Transport Problem. Retrieved from http://arxiv.org/abs/2203.00813\n[3] \"On Unbalanced Optimal Transport: Gradient Methods, Sparsity and Approximation Error\".\nQuang Minh Nguyen, Hoang Huy Nguyen, Lam Minh Nguyen, Yi Zhou"
    },
    {
      "summary": "Current SB solvers often have complex neural parameterization and time-consuming training procedures due to minimax. This work proposed a novel parameterization technique and a non-minimax training method to bypass the issues above, with the aim at offering a lightweight and easy-to-use SB baseline.",
      "strengths": "Strong motivation: the complex neural parameterization and time-consuming training procedures do hinder the application of current SB methods.\n\nClear presentation: the background knowledge and preliminaries are clearly presented. The connections with important references, such as (Tong et al., 2023), are highlighted and summarized in Table 1. The narrative is highly structural, with words and paragraphs to offer overviews of important sections.\n\nSoundness: the work seems highly self-contained: learning objectives, training/inference strategies, and theoretical properties. Limitations are also well discussed in appendix, some of which are examined through experiments.",
      "weaknesses": "1. Equation (5) seems incomplete or not well-defined. The equation is a crucial part of the methodology, and its clarity is essential for both understanding the method and replicating the results.\n\n2. It is pointed regarding other SB solvers that `they expectedly require time-consuming training/inference procedures.`. The authors state that their method avoids `time-consuming max-min optimization, simulation of the full process trajectories, iterative learning, and MCMC techniques,` which are commonly employed in existing solvers. While this claim holds conceptual interest, empirical evidence to substantiate this would significantly strengthen the paper.\n\nIn summary, although there exist some issues with this work, most of them seem correctable. I look forward to seeing the author feedback and the revised manuscript."
    },
    {
      "summary": "This paper proposes a fast Schrodinger Bridge (SB) solver based on the parameterization of Schrodinger potentials. SB is a dynamic version of the Entropic Optimal Transport (EOT), and there exists plenty of solvers for these problems, they requires complex parameterization by the form of neural networks / or is sensitive with the entropy regularization parameters, and thus are costly in terms of evaluation on larger scale datasets. The authors of this work propose a solution to this problem by consider a settings where the continuous SB is associated with a Wiener prior (as a reference measure). The key idea is to consider the parameterization of the Schrodinger potential as a mixture of Gaussian, and rewrite the SB optimization objective following this, with easy to compute mean and (scalar diagonal) covariance matrix. The authors provide some theoretical analysis of their algorithm, along with empirical demonstrations on synthetic dataset and realistic dataset (single cell data population dynamic and image-to-image translation).",
      "strengths": "- Algorithms based on well-studied theory of SB and EOT.\n- The paper is well-written with clear structure.\n- Well-motivated problem: lightweight solver is much needed for problem that usually requires heavy computational power.",
      "weaknesses": "* **Novelty of the paper:** it seems like the paper is just a combination of the two previous works [1, 2]. Admittedly, the authors have clearly elaborated in their paper of such cases, but I do not see the clear novelty in the methodology part. Even on the proof of the universal theorem of the Gaussian mixture model, one can see that half of it is straightforward calculation from the two aforementioned paper.  \n* **Unclear benefit the of LightSB solver in realistic setting:** for the single cell dataset using W1 distance as a metric, LightSB's performance leaves some gaps with the two best methods, however I do not see author's comment about this. The other benchmark on unpaired image-to-image translation is very hard to judge, as for this task there is no quantitative metric to compare with other solvers. I do not know why the authors omit unconditional image generation tasks, as this is an important and popular benchmark that has FID as a standard metric. Morevoer, there have already exist results on some of the neural EOT solvers or the diffusion SB solver (using iterative proportional fitting)/diffusion SB matching (using iterative Markovian fitting), or flow matching/rectified flow in this task, so it is easy to compared with the baseline. \n*  **Questionable theoretical result:** in the proof the universality of the Gaussian mixture parameterization for SB (theorem 3.4), in the paragraph below equation (29), I fail to understand why the authors wrote \n\n> \"Besides, it also has scalar covariances of its components because multiplier $exp(\u2212 |x_1|^2 /2\\epsilon )$\u2019s covariance is scalar itself\" \n\n, but what I understood in the paper's settings is that $x_1 \\sim \\pi_1$ is an unknown distribution, without assumptions on its parametric form. This is a key argument, as without it the factorization failed to be what the authors claimed in the statement of the theorem, I hope the authors could clarify this to me, otherwise it would be a hole in the proof of an important analysis of the paper.    \n\n[1] Petr Mokrov, Alexander Korotin, and Evgeny Burnaev. Energy-guided entropic neural optimal transport. arXiv preprint arXiv:2304.06094, 2023.  \n[2] Nikita Gushchin, Alexander Kolesov, Alexander Korotin, Dmitry Vetrov, and Evgeny Burnaev. Entropic neural optimal transport via diffusion processes. In Advances in Neural Information Processing Systems, 2023a."
    },
    {
      "summary": "This paper proposes a novel method to approximate the Schr\u00f6dinger Bridge problem. Following the connections of the Schr\u00f6dinger Bridge problem to entropic optimal transport, the authors want to learn the true entropic OT plan. They parametrize the optimal entropic OT plan as the product of two probabilities, the source measure $p_0$ and a conditional plan $\\pi(x_1|x_0)$ (up to a normalization constant). Following this novel parametrization, they show that they can optimize the parametrized plan without the knowledge of the true entropic OT plan. To deal with the normalization constant, they use a mixture of Gaussian representations to get a tractable form of each of the components. They then explain the inference and training procedure. Finally, they perform several experiments to show the practicability of the proposed method (2D synthetic data, an EOT benchmark, single-cell dynamics and unpaired image-to-image experiment).",
      "strengths": "i) The parametrization of the entropic OT plan is novel (to the best of my knowledge) and interesting.\n\nii) The derived loss is very interesting as learning the parametrized model does not require knowing the true entropic OT map.\n\niii) The discussion on the normalization constant and the proposed parametrization (with a mixture of Gaussians) to overcome this issue is appealing (even if the computation of the normalization constant is based on existing works).\n\niv) The method has been tested in different experiments.\n\nv) The paper is clear and easy to read.",
      "weaknesses": "1. In the single-cell experiments, the competitor results are taken from another paper [Tong et al., 2023]. As the evaluation is performed on a leave-one-domain out and training on the others, it is questionable that the training procedure was the same. I believe that some of these competitors should be reproduced by the authors to ensure that the training setting is similar, especially as the unpaired image-to-image experiment is a quantitative experiment without competitors.\n\n2. The experiments were performed in a relatively small dimensional setting: the unpaired image-to-image experiments used a pre-trained feature extractor of dimension 512, the single-cell data experiment used the representation of the 5 whitened principal components (ie dimension of 5), and the EOT benchmark data were (at most) of dimension 128. Therefore, the question of the performance of the proposed method in high-dimensional data is legitimate, especially as the authors use a mixture of Gaussian representation.\n\n3. Little is said about the statistical estimation (with respect to the data dimension and sample size) of the proposed parametrization to the true entropic OT plan $\\pi^\\star$. It is known that it also suffers from the curse of dimension [2,3].\n\n4. Some related work is missing and should be discussed and mentioned [1,2,3,4]\n\n[1] Stochastic optimization for large-scale optimal transport, Genevay et al, Neurips 2016\n[2] On the sample complexity of entropic optimal transport, Rigollet et al.\n[3] Minimax estimation of smooth optimal transport maps, Hutter et al.\n[4] Learning with minibatch Wasserstein, Fatras et al., Aistats 2020"
    },
    {
      "summary": "The paper proposes a fast, light-weighted, solver for Schr\u00f6dinger bridge by using a Gaussian mixture parametrization on the energy potentials. This simplifies computation of the static EOT map that would otherwise be intractable, resulting in efficient learning process. Extensive experiments are conducted on low/mid dimensional benchmark, single-cell dataset, and unpaired image translation in latent space.",
      "strengths": "- The proposed method is notably simple and elegant. It effectively combines key insights from previous works and elevates them to address a significant problem within the SB community.\n\n- The paper is well-written, and the thorough comparison to related works, especially in Table 1, is particularly valuable. Additionally, the comprehensive discussions in the appendix about limitations and broader impacts are appreciated.",
      "weaknesses": "- The proposed method on image dataset requires pretrained latent space (512 dimension) that is already structurally informative.\n\n- On the discussion of tractable / real-world SB given pairing, a few important references such as \"Aligned SB\" and \"image-to-image SB\" are missing. \n\n- Given that the proposed method is computationally light weighted, it'll be beneficial to have some quantitative comparison (actual runtime, memory etc) to prior works.\n\n- All \"Schrodinger\" should be changed to Schr\u00f6dinger."
    }
  ],
  "Inverse Approximation Theory for Nonlinear Recurrent Neural Networks": [
    {
      "summary": "The authors provides an inverse approximation theory for a certain class of nonlinear recurrent neural network (RNN), showing that, for the class of nonlinear RNN to approximate the sequence, the sequence should have an exponential decaying memory structure. It demonstrates the difficulties of RNN in learning and representing a longer-time dependence in the data.",
      "strengths": "The paper is relatively well written and it extends the previous theories in linear RNN. While heuristically it is not difficult to show that a popular RNN, such as LSTM or GRU, has an exponentially decaying memory, by approximating the internal state of the RNN with a relaxation equation, this manuscript provides a robust proof about such behavior.",
      "weaknesses": "While it the manuscript is clearly written with well defined problem set up, my concern is the fit to the scope of ICLR. On one hand, I believe that the analysis and definitions provided in the manuscript can be of interest in developing theories about RNN. On the other hand, the scope of the manuscript is too narrow and it is unclear what insight this study provides to benefit a broader class of RNNs or Deep Learning in general. I believe that the authors need to consider giving more insights from their results for a broader context. The stable parameterization section is not very convincing. Can you give a more concrete example about the effects of the reparameterization? Possibly, using a dynamical system or real data?"
    },
    {
      "summary": "This paper demonstrates a Bernstein-type result for nonlinear Recurrent Neural Networks, meaning that it characterizes the type of functions that can be approximated by RNNs. The result shows that if a target sequence can be approximated by a RNN, then it has an exponentially decaying memory (for a notion of memory made precise in the paper). A similar result was proven in the literature for linear RNNs and this paper extends the result to the non-linear case.",
      "strengths": "The paper is well presented and pleasant to read. Extending previously known results to the non-linear case is interesting and brings added value. It gives a theoretical framework to understand the common saying that RNNs are unable to learn long-time dependencies in time series. A modification of the parametrization of RNNs is proposed to remedy this problem.\n\nTechnically, the paper introduces a new notion of memory that holds for nonlinear functionals and which extends the linear case, as well as a notion of stable approximation (which corresponds to the existence of a ball of parameters with uniformly good approximation properties). This proof technique is interesting and could perhaps be applied to other architectures.\n\nExperiments support the theoretical findings.",
      "weaknesses": "I have no strong reservations about the paper. I do have a question about Figure 3, which I did not understand. I am willing to raise the rating should this interrogation be answered. [Update on Nov. 18: the authors clarified this point in their comments below, and I updated my rating accordingly].\n\nI do not understand the filtering part of the experiment. Since we know from the theoretical results that RNNs can only represent exponentially decaying functions, the teacher models should all be exponentially decaying? So why is any filtering required? Furthermore, if indeed filtering is required, is it also performed before plotting the left-side plot? Otherwise we should be seeing some non-converging data points?\nAs a consequence, I am wondering if the takeaway of the experiment should be \u201cstable approximation implies exponential decay\u201d or rather \u201cexponential decay implies stable approximation\u201d.\n\n**Minor remarks that do not influence the rating**\n+ Page 5: I found the paragraph above Definition 3.2 hard to follow. I am not sure to understand the reason for introducing the terminology  \u201cqueried memory\u201d. The experiments of Appendix B are important in my opinion because they suggest that RNNs may share the exponential decay property with LSTMs, but not Transformer. But I do not understand why they are introduced at this point in the paper, and not later on, e.g. in the conclusion.\n+ Page 7: the proof sketch is difficult to follow, and I am not sure it brings significant added value. If you need additional space in the main paper, I\u2019d suggest putting the proof sketch in the Appendix just before the proof.\n+ Page 7, line -2: \u201cIn order\u201d -> \u201cin order\u201d\n+ Page 8: \u201capproaching 0\u201d -> \u201capproaching 0 on the negative side\u201d?\n+ Page 8: \u201cTake exponential\u2026\u201d -> \u201cTaking exponential\u201d\n+ Page 8, figures 2 and 3: please avoid using plus sign and arrow inside the text. Consider replacing it by \u201cand\u201d and \u201cimplies\u201d.\n+ Page 16: \u201cacivations\u201d -> \u201cactivations\u201d\n+ Page 16: could you briefly explain or give a reference for Lyapunov equations?"
    },
    {
      "summary": "The paper introduces an inverse approximation theorem for non-linear recurrent neural networks, extending known, results for the linear case. After introducing new tools that enable analyzing nonlinear dynamics, it shows a somewhat negative result that nonlinear RNNs suffer from the same curse of memory as linear RNNs. Finally, the authors suggest a type of reparametrization that could possibly enable stability and good approximation properties.",
      "strengths": "As an outsider to the field, the paper introduces/uses formal concepts that I find insightful:\n\n- The memory function that is used in the paper mathematically characterizes the intuitive behavior of RNNs.\n- The notion of stable approximation is an interesting proxy for how a function can be learned by gradient descent which, from my very limited knowledge, seems to be rare in approximation theory.\n\nOverall, I found overall the paper well written, in a way that is accessible to a decently large audience.",
      "weaknesses": "I am not qualified enough to identify the weaknesses of the paper."
    },
    {
      "summary": "This paper investigates the inverse approximation theory for nonlinear recurrent neural networks (RNN) and extends the curse of memory from linear RNNs to nonlinear RNNs. The contribution of this paper can be summarized as follows.\n1. Authors define the memory of nonlinear RNNs, which is consistent with the memory of linear RNNs.\n2. Authors introduce a notion of stable approximation.\n3. Authors prove a Bernstein-type approximation theorem for nonlinear functional sequences through nonlinear RNNs, which says that if a target function can be stably approximated by nonlinear RNNs, then the target function must have exponential decaying memory.\n4. Based on the theoretical result, the authors propose a suitable parameterization that enables RNN to stably approximate target functions with non-exponential decaying memory.",
      "strengths": "1. To the best of my knowledge, this is the first paper studying the inverse approximation theorem for nonlinear RNNs.\n2. The approximation of neural networks determines the existence of a low-error solution and has been widely investigated. The inverse approximation theorem, which is harder and less studied, concerns the efficiency of approximation, which is a crucial aspect of utilizing neural networks in practice.\n3. The nonlinearity is of central interest in empirical implementations and requires more challenging proofs.",
      "weaknesses": "1. The definition of memory for nonlinear RNNs is consistent with that for linear RNNs, but the rationality of the proposed definition needs more explanations. The authors review the definition of memory for linear RNNs and observe that if the memory decays fast, then the target has short memory, and vice versa. The observation can be obtained directly from the definition and satisfies the intuition of memory. But for the memory for nonlinear RNNs, authors motivate it from a derivative form. This motivation is too mathematical and lacks an intuitive explanation. What is the relationship between the decaying speed of memory and the dependence of the target function on early inputs?\n2. The parameterization motivated by the theories should be explained in more detail. The parameterization is based on an important claim that the eigenvalue real part being negative leads to stability, which is not explained in section 3. This makes the logic in section 3.4 incomplete and the parameterization hard to understand. Can authors provide an intuitive explanation for this?\n----\nThe authors' answers are convincing and I have updated my ratings."
    }
  ],
  "RNA-Protein Interaction Classification via Sequence Embeddings": [],
  "Neuroformer: Multimodal and Multitask Generative Pretraining for Brain Data": [
    {
      "summary": "This manuscript introduces a multi-modal, multi-task generative pretrained transformer as a tool for analyzing the increasing volume of data generated by large-scale experiments in system neuroscience. The goal of this tool is to create a better neural spiking model while taking external variables into account. \u00a0In particular, they applied the Perceive IO architecture (Jaegle et al. 2021) to the neural domain and modified it accordingly. During the process of decoding, they also developed feature backbones, which enabled the specialized architecture to track the activity of individual neurons. Their loss function has a component that deals with alignment as well as one that deals with spike creation. The alignment component explicitly enforces\u00a0representational commonalities among biologically significant features. The causal spike modeling is used by the spike generation component to do an autoregressive decoding of brain spikes. They used a simulated dataset in addition to two different two-photon calcium imaging datasets in order to verify the accuracy of this neuroformer. They demonstrated, with the simulated dataset, that the neuroformer is capable of recovering the hub-neuron structure that is comparable to the ground truth. Using the calcium imaging dataset, they were able to demonstrate that the suggested neuroformer performed better than GLM when it came to creating neuronal spikes. They also showed that a pretrained neuroformer has more accurate predictive features of mouse behaviors than baselines like Lasso regression, GLM, MLP, and GRU. In addition to this, they presented the results of an ablation investigation, which demonstrated that each module contributes progressively to predicting eye position.",
      "strengths": "The loss function combines multi-task representation loss with two losses relevant to generating neural spikes. To the best of my knowledge, this particular application of multi-task learning to modeling neural activity is new.\u00a0\n\nThe feature backbones in the Neurofomer are able to dissect single neurons. A common limitation of previous machine learning approaches to modeling population activity is that they lose single neurons. This feature seems to circumvent such a limitation.",
      "weaknesses": "Lack of comparison with strong baselines is my main concern for this submission. Prior to this paper, there were a couple notable publications that leveraged the transformer architecture to generate neural spikes. Albeit those most well-known ones are single modality only, it is still worth a comparison in terms of neural modeling. This work only showed its comparison with simple baselines (MLP, GRU) when it compared the quality of neural spike generation. If this neuroformer does not perform as well as other transformer-based architectures, I would hope the authors may include more elaborated discussion on whether the appeal of cross-modality representation outweighs its limited performance.\u00a0\n\n1) Liu 2022 Seeing the forest and the tree: Building representations of both individual and collective dynamics with transformers\n\n2) J. Ye and C. Pandarinath, \u201cRepresentation learning for neural population activity with Neural Data Transformers,\u201d Neurons, Behavior, Data analysis, and Theory, Aug. 2021\n\nThe F1 scores in Figure 5 are rather low at their absolute values. It would be helpful if the authors put the F1 score in perspective (why is this F1 score indicating good performance?). Is it possible for the authors to comment on the dip of the F1 score after adding the video modality in the Visnav, Lateral dataset?\u00a0\n\nThe correlation difference in Fig 3C is also low in comparison with GLM. Such a comparison will be a lot stronger if it is versus another transformer architecture or more elaborate architecture that is capable of expressing neural activity fully.\u00a0\n\nSpeed is misspelled as \u201cspped\u201d"
    },
    {
      "summary": "The paper presents a multi-modal Transformer-based pretraining paradigm for learning joint representations of neural activity, behavior and sensory inputs. It mainly follows recent work in vision-language modeling and adapts these approaches to the neuroscience setting. The paper shows that (a) the attention maps can reveal the circuit structure in a simulated toy dataset with a few hub neurons, (b) it can predict neural activity based on past activity and stimulus slightly better than a generalized linear model, and (c) it can decode running speed of a mouse from neural activity in a few-shot manner.",
      "strengths": "+ Promising self-supervised learning paradigm for large-scale, multi-modal neuroscience data\n + Nice set of experiments from simple toy model with known ground truth to real, large-scale data\n + Overall well-written and mostly easy to follow (with some exceptions)",
      "weaknesses": "1. Weak baselines in Figs. 2+3\n 1. Small effect in Fig. 3\n 1. Architecture (especially decoder) not entirely clear from paper"
    },
    {
      "summary": "This paper proposes a multimodal, multitask generative pre trained transformer model called Neuroformer. This model uses an arbitrary number of modalities, such as neural responses,  external stimuli, and behavior, to perform downstream tasks. The authors apply this model to predict simulated neural circuit activities and behavior of a mouse from its neural recordings, where four modalities are used: neural responses, video, speed, and eye position. They also perform ablation studies to explore the impact of each model component.",
      "strengths": "* Propose a multimodal, multitask transformer-based model for neural data modeling.",
      "weaknesses": "* In the experiments, the Neuroformer is only evaluated in terms of behavior prediction, which is not enough in evaluating neuroscience tasks. The choices of models (GLM, GRU, Lasso Regression) for comparison are also not convincing to me. One suggestion is to follow the evaluation criteria in [Neural Latents Benchmark](https://eval.ai/web/challenges/challenge-page/1256/overview) and compare the Neuroformer with the top leading models there, such as S5 and LFADS on the multimodal calcium imaging datasets. It is critical to see whether the proposed model is a solid technical innovation with practical influences in neuroscience research.\n\n* As the authors mentioned in Appendix A, the Neuroformer has poor results (Figure 10) in low-dimensional latent space learning. One possible reason is that the transformer-related neural networks are too expressive so that good dynamics in latent space are no longer necessary. But in terms of interpretability, neuroscientists prefer to observe meaningful latent space in many experimental scenarios, which may be more important than a good behavior prediction performance."
    },
    {
      "summary": "In the paper \"Neuroformer: a multimodal, multitask GPT framework for brain data at scale\", the authors suggest a transformer-based architecture (Neuroformer) for fitting high-dimensional neural spike train recordings, that can incorporate a CLIP-like contrastive learning objective to use visual stimuli and/or behavioural recordings. The authors argue that Neuroformer can slightly outperform classic GLM models for spike prediction and strongly outperform simpler models for behavioural prediction.",
      "strengths": "The paper is interesting because it applies modern transformer architectures to modeling neural data, and shows competitive results.",
      "weaknesses": "Overall I thought that the paper would perhaps be more suited for a computaional neuroscience journal or for NeurIPS that traditionally has some amount of comp neuro papers. At ICLR, this topic is an outlier, as there is very little (next to none) computaitonal neuroscience there.\n\nWhereas the paper is generally well-written, I found many model details not sufficiently clear (examples below)."
    }
  ],
  "MuSc: Zero-Shot Industrial Anomaly Classification and Segmentation with Mutual Scoring of the Unlabeled Images": [
    {
      "summary": "This paper presents a MuSc model for zero-shot AC/AS. The method exploits the normal and abnormal information implicit in unlabeled test images without any training or prompts, A Mutual Scoring Mechanism (MSM) is proposed to assign abnormal scores to each other using unlabeled test images, and an optimization method based on constrained image-level neighborhoods (RsCIN) for image-level anomaly classification. The method shows better performance on MVTec AD and VisA datasets.",
      "strengths": "1. The idea that normal image patches can be found in a relatively large number of similar patches in other unlabeled images, while abnormal image patches have only a small number of similar patches, is novel. The authors were able to accomplish this task simply by using a test dataset and utilizing the test images to score each other without any training or prompting.\n\n2. The authors considered the size of the anomalies in different datasets and used patch tokens with multiple aggregation degrees to obtain high-quality anomaly scores even when using a simple distance measure.\n\n3. The authors found that the image-level features satisfy the conditions of high-dimensional manifolds, and designed RsCIN based on manifold learning to optimize the pixel-level anomaly classification results, and experimentally verified the effectiveness of the module, proving that the proposed RsCIN module can further improve the performance of the existing methods.",
      "weaknesses": "Even without training or prompts, the method requires long inference times and high memory costs. While the authors provide a solution to increase speed and reduce memory by dividing the test set into subsets, this also reduces performance by a small margin. Are there any other approaches that could have been considered to solve the problem?\n\nThe authors should provide more detailed theories for MSM and RsCIN."
    },
    {
      "summary": "The paper proposes an anomaly detection and segmentation method utilizing unlabeled images from a test set. It is assumed that a set of images with some form of anomaly is available for inference and it is claimed to be zero shot. \nThe method starts with computing a representation for every image patch through aggregating (pooling) token features generated by ViT at different layers. An anomaly score for each patch is then computed by comparing the aggregated patch token with that from each image in the *test* set. Further heuristic tricks are applied to refine this score and the final pixel level anomaly score is given by the max of the refined score. \nThe method then produces an image level classification by defining a weighted graph over *test* set images where weights are defined by the class token generated by ViT. The image level classification score is the determined by by some graph operations that were not explained well. The method was tested on public datasets and compared with solid baselines.",
      "strengths": "I am afraid I was not able to find a notable strength of this paper.",
      "weaknesses": "1. Technical soundness: The method relies heavily on availability of a set of images in order to compute the anomaly scores. I dont think this emulates the practical scenario of anomaly detection in industry. The more realistic scenario is a method is required to classify and segment the anomaly given one image. Using test set images to produce the output on the same set of images also does not conform with the scientific procedure. I dont think this is the definition of a zero shot method either. The setting sounds completely unreasonable to me.\n\n2. Contribution: The algorithm appears to be a set of heuristic tricks applied in a sequence. There is not a solid technique that is novel, elegant, theoretically justified and of broad interest.\n\n3. Clarity: None of the techniques were adequately explained as to why they are being performed and why it makes sense (intuitively or conceptually) to apply them. Just stating the process does not qualify as a good scientific exposition."
    },
    {
      "summary": "The paper introduces MuSc, a novel zero-shot framework for industrial anomaly classification and segmentation. It utilizes cues from unlabeled test images and combines local patch tokens with a mutual scoring mechanism. The method notably outperforms existing zero-shot approaches and rivals many few-shot and full-shot methods.",
      "strengths": "1. The MuSc framework introduces a novel approach to zero-shot anomaly classification and segmentation, particularly in the industrial domain. The method of leveraging implicit cues from unlabeled test images for anomaly detection is an innovative concept.\n\n2. The empirical results demonstrate a substantial improvement over existing zero-shot approaches and competitiveness with few-shot and full-shot methods.\n\n3. The approach holds significant potential for industrial applications, where anomaly detection is crucial but training data is often scarce or expensive to obtain.",
      "weaknesses": "In Table 3, the ablation study of LNAMD with different aggregation degrees \\(r\\) raises a question regarding the effectiveness of combining aggregation degrees. Specifically, it's unclear why the combination of \\({3, 5}\\) performs worse in the anomaly classification (AC) task than using \\({3}\\) alone. This observation seems to contradict the paper's claim that using all aggregated patch tokens with different degrees is beneficial for detecting anomalies of various sizes. Based on this claim, one would expect the combination of \\({3, 5}\\) to outperform either \\({3}\\) or \\({5}\\) individually. This inconsistency warrants further clarification or investigation to reconcile the results with the stated claims."
    },
    {
      "summary": "This paper addresses zero-shot anomaly classification (AC) and segmentation (AS) with following contributions:\n1) Using unlabeled test images for AC/AS.\n2) A new mutual scoring mechanism for identification of abnormal patches.\n3) SOTA performance, significantly outperforming existing zero-shot methods",
      "strengths": "The claimed contributions summarized above.",
      "weaknesses": "1) This paper makes the assumption that access to the entire test dataset is available. This allows for a direct application of the proposed mutual scoring mechanism. However, for zero-shot settings, such an assumption is too restrictive.  Access to test data is mostly very limited in real-world scenarios, especially for zero-shot.\n\n2) The method heavily relies on many heuristic design choices. It consists of a three-step pipeline which depends on many hyperparameters for feature representation, mutual scoring estimation, and classification rescoring. The paper poorly presents sensitivity of performance to optimizing all these parameters.\n\n3) Overall technical novelty seems incremental, since the method incorporates well-established multiscale features (Sec 3.1), norm-2 distance (Sec 3.2), and clustering (Sec 3.3). \n\n4) Since the method lacks a training phase and directly generates results based on estimating the test-set statistics, inference is likely to be much slower than in previous approaches. I was not able to find a report/comparison of the inference times."
    },
    {
      "summary": "This paper targets industrial zero-shot anomaly detection. Leveraging mutual scoring on unlabeled data, the proposed method achieves SOTA performance on several well-known industrial anomaly detection benchmarks.",
      "strengths": "1.\tThe paper is well written, with clear architecture.\n2.\tThe motivation is insightful, with clear ablation studies to show its effectiveness. The motivation and the proposed method are well-matched.",
      "weaknesses": "1.\tThis paper employs a methodology that utilizes unlabeled test images to collectively measure anomaly scores, differing somewhat from the traditional zero-shot recognition setting. In the conventional approach, each image is independently evaluated, such as WinCLIP. This difference in evaluation methodologies can result in a somewhat unfair comparison.\n2.\tThe underlying concept is reminiscent of [a], which presumes homogeneous input texture and identifies image regions that disrupt this homogeneity as anomalies. In other words, if a patch significantly differs from its neighboring areas, it's deemed an anomaly.\n3.\tThe approach of jointly measuring anomaly scores across entire datasets aligns closely with [b]. While not mandatory, it would be beneficial for the authors to discuss or draw comparisons with [b].\n\n[a] Aota et al. \"Zero-shot versus Many-shot: Unsupervised Texture Anomaly Detection.\" In WACV 2023.\n\n[b] Li et al. \"Zero-Shot Batch-Level Anomaly Detection.\" arXiv, February 2023."
    }
  ],
  "Input-gradient space particle inference for neural network ensembles": [
    {
      "summary": "The paper proposes a novel method for ensembling deep models that ensures diversity of the ensemble members. The paper continues the line of work in particle-based variational inference transforming the repulsion step of this approach into an input gradient space. This is different from the existing works that have done this step in weight and function spaces.",
      "strengths": "* A novel method for an important problem of ensembling\n* Thorough empirical evaluation and comparison to the existing methods\n* Drawing connections with the existing methods\n* The paper is mostly well written and easy to follow\n* Runtime analysis presented",
      "weaknesses": "* Some presentation unclearness (see details below)\n* Some transformations between theory in Section 3 and steps in Algorithm (in Appendix) are not obvious\n\n\n1. What corruption is considered? CIFAR-10/100-C datasets have several types of corruptions each of which has several level of severity of corruptions. No confidence intervals (+-) for corruption results. \n2. Section 3.1 doesn't address that the target distribution \\pi is not available, or am I missing something? \n3. It would help to clear some confusion of how Algorithm comes in place if steps in Algorithm would be linked to equations in Section 3. \n4. Section 3.4. \"However, in practice we found no performance degradation nor convergence issues in our experiments\" - though the convergence issues can easily be observed, in order to see no performance degradation one would need to compare the performance with and without mini-batches. This experiment is not presented in the paper (including Appendix). \n5. Though the code is provided, some implementation details in text are missing. For example, ECE computation details such as a number of bins. Or details of OOD experiments: what portion of OOD data (CIFAR-100 for CIFAR-100 and vice versa) was used. \n6. No reference for CINIC10 dataset"
    },
    {
      "summary": "This paper points out that while the repulsion in the existing weight-space or function-space repulsive deep ensembles has been theoretically well-motivated, it does not lead to a practical performance improvement compared to vanilla deep ensembles. Rather than relying on repulsion in weight or function space, the authors employ a kernel comparing input gradients of particles and propose First-order Repulsive Deep Ensembles (FoRDE). Experimental results clearly indicate that FoRDE outperforms baseline methods, particularly when dealing with corrupted data.",
      "strengths": "1. I have experienced that although repulsive deep ensembles are theoretically well-grounded, they do not result in performance enhancements in practice. In this regard, this paper is well-motivated, as it states, \"Neither weight nor function space repulsion has led to significant improvements over vanilla DEs.\"\n2. The paper provides a comprehensive overview of the literature concerning repulsive deep ensembles. Also, the proposed approach is meticulously detailed in a step-by-step manner, as well as its practical considerations.\n3. The connection to the EmpCov prior (Izmailov et al., 2021) further clarifies why the proposed FoRDE-PCA algorithm performs well for data under common corruptions.\n\n---\nIzmailov et al., 2021,  Dangers of Bayesian model averaging under covariate shift.",
      "weaknesses": "Despite the critique that neither weight nor function space repulsion yielded significant improvements compared to vanilla DEs, the FoRDE algorithm introduced in this context still did not result in a substantial performance enhancement over vanilla DEs. In particular, FoRDE-Identity demonstrates a performance similar to that of vanilla DE, while FoRDE-PCA excels in performance under corruption but significantly diminishes its in-distribution performance.\n\nThe authors seem to have recognized this aspect; \"Hence, we believe that the optimal lengthscales for good performance on both clean and corrupted data lie somewhere between unit lengthscales (the identity) and using the inverse square root eigenvalues as lengthscales.\" For this paper to be considered complete, it should not just acknowledge such ideal lengthscales but also offer experimental evidence of their practical identification."
    },
    {
      "summary": "This paper is concerned with adapting particle based variational inference for improved training of neural network ensembles.  The authors attempt to circumvent problems that have affected previous attempts to use particle based variational inference for ensembles, with a lack of effective repulsion in weight space (intended to promote functional diversity) chief among them.  This paper proposes instead to enforce diversity in the input gradients rather than in weight space, by using Wasserstein gradient descent along with an RBF kernel defined over the input gradients to guide the particles during training.  They compare against deep ensembles and other BNNs on accuracy, calibration, and robustness to covariate shift.",
      "strengths": "- The paper is very well written.  The potential advantages of moving to input gradient based diversity management are well introduced & well motivated, and the explanations are largely self-contained, which is no small feat considering page restrictions for conferences.\n- In particular, the main contribution section (section 3) is *so* well written.  It takes time to lead the reader from the wider view of Wasserstein gradient descent, to input space gradients, and the more narrow questions of choice of kernels, and their tradeoffs.  Of all the papers I reviewed, this was by far the most enjoyable and informative to read.  Bravo for taking the time to write so clearly.",
      "weaknesses": "- One thing I often worry about is that the experiments are performed only in the vision domain, on over-hygenic datasets.  While I don't want to discount the amount of work needed to extend to other domains, projects like [WILDS](https://wilds.stanford.edu/) make this easier, and build confidence that demonstrated success isn't due to some quirk of CIFAR datasets.\n- One other complaint that to the authors' credit they highlight in section 3.5 is the cost of computing FoRDEs.  At a 3x computational premium to DEs, the penalty paid in compute seems to be the largest drawback of FoRDE with respect to DEs.  Do the authors have any ideas for reducing this burden? DEs themselves are expensive in both space and time to compute."
    },
    {
      "summary": "Most prior research employing particle-based variational inference (ParVI) has proven to be inefficient and has not significantly improved performance. To tackle these issues, this study presents a new ParVI approach known as the First-order Repulsive Deep Ensemble (FoRDE), which integrates repulsion principles into the realm of first-order input gradients.",
      "strengths": "- The idea of incorporating repulsion into first-order input gradients(not a function space or a weight space repulsion which are quite common in Bayesian Neural Network literature) to enhance functional diversity is new to the community and intriguing.\n- The paper is well-written, ensuring it is easy to read and understand.",
      "weaknesses": "- The scale of experiments are quite small to show the effectiveness of FoRDE.\n- The overall performance gain compared to other baselines looks quite marginal for the out-of-distribution datasets, especially for the TinyImageNet which is the largest dataset. And shows lower performance compared to the other baselines for the in-distribution datasets.\n- Having empirical or theoretical evidence to demonstrate the effectiveness of FoRDE in enhancing input gradient diversity would be beneficial.\n- Providing empirical results that illustrate how the improved input gradient diversity effectively changes into enhanced functional diversity in deep neural network scenarios would be valuable. \n- Additional hyperparameters for the kernel would be another burden for this method."
    }
  ],
  "Quantifying Network Similarity using Graph Cumulants": [],
  "Accurate and Scalable Estimation of Epistemic Uncertainty for Graph Neural Networks": [
    {
      "summary": "The paper proposes a training framework for GNNs that is designed to improve the intrinsic uncertainty estimates. The adopted strategy is to adapt the principle of stochastic data centering to graph data. This involves introducing novel graph anchoring techniques. The paper demonstrates that the methodology can support partially stochastic GNNs. Experimental results in the paper suggest that the partial stochasticity is sufficient; it also has the advantage of providing a mechanism for incorporating pre-trained models. The paper reports experiments investigating the impact of covariate, concept and graph size shifts and demonstrates that the proposed technique leads to better calibrated GNNs, both for node and graph classification. Additional experiments illustrate how the approach performs for out-of-distribution detection and generalization gap estimation.",
      "strengths": "S1. The paper introduces a novel approach for improving the intrinsic uncertainty estimates of GNNs by translating the stochastic centering strategy to the graph domain. This is non-trivial, both for node and graph classification.\n \nS2. The paper reports on experiments exploring (i) node classification under distribution shift (concept and covariate shifts); (ii) calibration under distribution shift for graph classification; (iii) how the approach impacts the calibration of more expressive models such as graph transformers. The experiments are thorough and examine multiple interesting questions. \n\nS3. The experiments support the interesting observation that the network does not need to be fully stochastic in order to provide improved uncertainty estimates. This paves the way to combine the p",
      "weaknesses": "W1.\tSome of the methods employed to translate stochastic centering to the graph domain appear somewhat heuristic, or are at least the text describing the methodology does not provide sufficient detail to perceive the design principles. For example, the node feature anchoring fits a Gaussian distribution, but there is no explanation as to why a Gaussian is selected and no discussion as to whether the mismatch between the fitted anchor distribution and the feature distribution has a negative impact or could be a concern. The text states that the introduction helps to \u201cmanage the combinatorial stochasticity induced by message passing\u201d but it does not elaborate on this to explain why or how. For the graph anchoring, there is a random shuffling of the node features over the entire batch. There is no discussion of this design choice \u2013 it doesn\u2019t seem obvious to me that this is the only thing one could choose to do (or the optimal). \n\nW2.\tDistilling the methodological contributions, we see that they involve: (i) node anchoring via fitting a Gaussian distribution and drawing an anchor from this fitted distribution; (ii) hidden layer anchoring by randomly shuffling the node features after the r-th layer. After these steps to construct appropriate anchors in the graph domain, there is effectively a standard application of the stochastic centering approach. The technical methodological contribution is thus not particularly substantial. On the other hand, the experiments are thorough and provide a good balancing contribution. \n\nW3.\tSome of the results are not presented in a particularly helpful manner and are not described or discussed in much detail. For example, the observations for node classification essentially boil down to \u201cour method works better\u201d. The table contains interesting elements such as the proposed method failing to improve (WebKB, CBAS \u2013 Concept) or substantially increasing (Cora) the ECE when combined with Dirichlet calibration. But there is no discussion of this. In general there is not a significant attempt to draw detailed conclusions from the obtained results \u2013 similar comments apply to Section 5.3 where again the conclusions are \u201cboth pretrained and end-to-end G-\u2206UQ outperform the vanilla model on 7/8 datasets\u201d and \u201cG-\u2206UQ variants improve OOD detection performance over the vanilla baseline on 6/8 datasets\u201d. Insights beyond \u201cworks better\u201d make a paper much stronger and more insightful. \n\nThere is room for improvement in some of the figures and the explanations of how they are being interpreted. Figure 2 is a particular case in point \u2013 L1, L2, L3, N/A are not clearly defined. The text states that \u201cREADOUT anchoring generally performs well\u201d but does not explain how we should interpret the figure to come to this conclusion. It\u2019s not obvious what \u201cperforms well\u201d means \u2013 what is an acceptable deterioration in performance. The behaviour over datasets and architectures differs considerably and should be discussed. \n\nIn terms of assessing the variability of performance, the paper reports standard deviations over a few trials, but does not make any attempt to assess the statistical significance of the results or to specify confidence intervals on the reported means."
    },
    {
      "summary": "The paper proposed a new training framework inspired by the stochastic anchored training in computer vision. In the framework two (partial) stochastic anchoring techniques (node feature anchoring and hidden layer anchoring) are designed for sake of the better uncertainty estimation and calibration in GNNs. The experiments on node classification and graph classificaiton validate the effectiveness of the framework.",
      "strengths": "1. The idea that introducing the anchoring training in GNNs is new and interesting.\n\n2. The paper conducted comprehensive evalutions on the calibration of GNNs under different settings. Specifically, the evaluation on calibration of GNNs under distribution shift is new and significant. The experimental results show that on most tasks the proposed method can achieve lower calibration error.",
      "weaknesses": "1.Since the paper focused on calibration and uncertainty estimation of GNNs. The concepts of calibration and uncertainty estimation of GNNs should be introduced in the paper.\n\n2.The paper doesn't provide sufficient discussion or theories to justify the methods provided in the paper. For instance,  in node feature anchoring why authors sample the anchors from the Gaussian Distribution? How to determine the value of $u$ and $\\sigma$?\n\n3.The paper claimed that the framework can improve the uncertainty estimation. However, how the method can improve the uncertainty estimation is still unclear.\n\n4.The organization of the experiment part is messy. More setup details should be clarified.\n\n5.Some typos in the paper and the title of Table 2 is missing."
    },
    {
      "summary": "This paper extends the \u2206-UQ model to graph learning to enhance intrinsic GNN uncertainty estimates. It proposes several training techniques to introduce partial stochasticity for node and graph-level classification tasks. Extensive experimental results validate the effectiveness of the proposed method. Interestingly, the graph-level training method can be applied to pre-trained graph models.",
      "strengths": "1. Extensive experimental results on various datasets and setups demonstrate the effectiveness of the proposed method.\n\n2. The introduced method is fairly simple and easy to understand. It seems that it can be applied to many existing models in a plug-and-play manner, making it flexible and extendable.\n\n3. The paper also explores the potential of applying this method to pre-trained models, which is an interesting aspect.",
      "weaknesses": "1. Some notations and definitions in the paper are unclear, such as the symbols used for samples and distributions, and the shape of tensors during concatenation.\n\n2. The experiments conducted focus solely on classification tasks. Consideration for regression tasks seems to be missing.\n\n3. There are limited performance gains observed with the use of pre-trained models.\n\nThese points are further detailed below."
    }
  ],
  "UniTabE: A Universal Pretraining Protocol for Tabular Foundation  Model in Data Science": [
    {
      "summary": "The paper proposes a transformer model for table classification and regression. The authors curated a large set of Kaggle datasets for training and evaluation. The central component of the proposed architecture is the TabUnit, which embeds data type, column name and column value respectively, and fuses as well as links the embeddings via gates. The authors propose to use a Transformer encoder and LSTM as decoder. The model is pre-trained by different strategies, namely cell and column masking and siamese triplet loss based on (partial-) row contents. Fine-tuning then entails predicting missing columns for a new dataset specialisied prompts for tasks such as question answering. The authors trained the Transformer on a self-collected dataset comprising 13 billion tables. The approach is evaluated for standard, fine-tuned as well as combined version (with XGBoost). Part of the evaluation is done for Kaggel tasks (12), where the approach is compared to XGBoost and TransTab. The rest of the evaluation is performed for 7 public datasets, where a larger amount of competitors is compared against. The results show that the proposed approach often achieves superior performance in terms of AUC/R2. In addition, multiple ablation studies also highlight, among others, robustness of the approach towards missing data.",
      "strengths": "The chosen topic of the paper is highly relevant to the venue, as tabular machine learning is, as argued in the paper, often overlooked but highly important in industrial applications.\n\nThe proposed approach to learn designated embeddings for data type, column name and values within the TabUnit is sensible and is different to existing approaches. In addition, the authors collect a large dataset for training their Transformer. It would be value adding to open-source such a model.\n\nThe work incorporates a large number of competitors in part of the evaluation (on open datasets): The authors include more than 10 baseline approaches of recent papers to compare the performance of their approach on a selected sample of open datasets. The chosen baselines, here, are well-chosen. The authors also provide ablation studies wrt to ther model design, highlighting the implications of omitting parts of the architecture. Lastly, the predictive performance of the approach is often better than the competitors.",
      "weaknesses": "It is unclear why only part of the evaluation was done for all Transformer-based competitors and why no regression results for a XGBoost regressor have been generated. It would have been highly insightful to have the same evaluation as for the open datasets.\n\nIt would also have been insightful to compare to a representative AutoML approach in order to see the boundaries of the approach. Of course, it does not have to be assumed that the model has to beat the best tuned models, but it should come close enough. \n\nWhile the results are promising, the gap between the competitors is often rather small. This is not to reduce the quality of the predictive performance, as the task at hand is very difficult and competitive, it just shows that competitors are already strong. \n\nWhile part of the evaluation is exhaustive in terms of baseline approaches, the evaluation contains relatively little open datasets compared to some of the competing papers. It would be beneficial to add more thorough design decisions to why the datasets have been chosen or why they are sufficiently representative.\n\nThe ablation study with respect to robustness is very interesting and highlights another benefit of the method - being able to deal with missing data. It would be intersting to know if this result holds over the other competitors too.\n\nA minor point (as many competitors were looked at): It would have intersting to see the performance of TUTA, as similar to the proposed approach, more refined embeddings are learned."
    },
    {
      "summary": "This paper presents UniTabE - a universal pretraining method for tabular data that consists of three components: tabular unit decomposition, encoding, and decoding. Compared to prior work, a key novelty of this model is that it is trained from scratch, and it involves innovations in terms of developing a TabUnit splitter and a simple decoder. The compilation of a large dataset for model training is also an interesting undertaking. The evaluation section provides a large number of experiments, showing the improvement brought by UniTabE on fine-tuned and zero-shot classification, regression tasks, and missing data prediction, whereas a series of ablations show the benefit of each aspect of the method.",
      "strengths": "S1) The paper is well-written and easy to follow. It provides information that should suffice to reproduce the presented method.\n\nS2) The methodological contributions are clearly indicated and well-justified by the paper. \n\nS3) The paper contributes a novel dataset and set of models, that should be interesting to practitioners working with tables.\n\nS4) The experimental section shows the benefit of this method against competitive baselines. Ablation studies are provided to ensure that the improvement comes from the expected method components.\n\nS5) To my knowledge, the undertaking to create such a foundational model for tables is novel. I agree with the authors that it can be very impactful given the amount of tabular data online.",
      "weaknesses": "W1) The paper lacks a discussion of the ethical and legal aspects of the data, including biases, copyright, and licensing.\n\nW2) The proposed method has a strong performance against the baselines consistently, but the baseline choices are inconsistent across the experiments. Can the authors motivate the reason for selecting certain baselines for each experiment, especially for Tables 4 and 5?\n\nW3) The paper talks about generalization to tables with more columns, but the generalization to tables with more complicated structures (e.g., multi-row and multi-column blocks) is not discussed. I am wondering how are such tables handled by the UniTabE.\n\nMinor:\n* Figure 3a is impossible to read - please enlarge the figure for the follow-up version, and ideally provide more guidance/commentary about it in the paper.\n* Typo: \"Expect for \" -> \"Except for\"\n* The commentary of Table 3 (page 9) is unclear, as it refers to \"a slight performance enhancement\", even though the two versions seem to perform more-or-less on par."
    },
    {
      "summary": "The paper delves into the application of large language models (LLM) to tabular data, presenting a novel approach utilizing the transformer model.\n\nTo address the unique characteristics of tabular data, the authors introduce \"TabUnit\", a method to amalgamate data type, column name, and cell values into comprehensive cell vectors. These vectors are then processed through transformer-based encoders and fed into a shallow LSTM decoder to regenerate tokens for the target label column.\n\nUpon pre-training the UniTabE model using 13B samples from Kaggle, the authors fine-tuned it on 12 chosen classification and evaluation datasets. The results reveal a significant improvement over prior methods that lack pre-training.",
      "strengths": "1. Quality:\nThe paper's quality stands out as it presents a model pre-trained on an impressively large tabular prediction dataset encompassing 13B samples. The authors conducted a meticulous evaluation of the model with contemporary tabular prediction baselines.\n\n2. Significance:\nThe results offered by the authors are particularly enlightening. While they tout the enhanced performance derived from pre-training on expansive tabular data, a closer examination (as reflected in Table 3) indicates only a slight uplift in downstream task performance post-fine-tuning, especially given the model's exposure to 13B samples. This subtle observation underscores the intricate challenges posed by tabular data prediction, taking into account its inherent heterogeneity and noise. It's pivotal to recognize that these findings could potentially delineate a performance benchmark for similar BERT-inspired methods in tabular prediction. This could galvanize researchers to delve deeper into innovative tabular modeling paradigms and re-evaluate data processing strategies for tabular prediction.",
      "weaknesses": "1. Evaluation:\nGiven the vastness of the training data, with 13B samples spanning 303 diverse domains, the evaluation datasets chosen seem somewhat narrow in scope. The comprehensive variety of training domains calls for an equivalently diverse evaluation to genuinely test the model's versatility and adaptability. Authors should consider widening their evaluation scope by leveraging an even more diverse array of tabular datasets.\n\n2. Results:\nWhile Table 3 does indicate that UniTabE marginally outperforms other baselines, the incremental gain appears to be disproportionate to the extensive computational overhead of the pre-training phase. It's noteworthy that the performance between a ground-up UniTabE (from scratch) and a pre-trained plus fine-tuned UniTabE are remarkably similar, prompting questions about the efficiency of the elaborate pre-training process.\n\n3. Method:\nThe paper's tab unit processing technique, although novel in its implementation, seems to draw heavily from precedents like TransTab and FT-Transformer. Additionally, the supervised strategies, such as multi-cell masking and contrastive learning, bear significant resemblance to methodologies detailed in prior research."
    }
  ],
  "Near-real-time monitoring of global ocean carbon sink": [],
  "Dichotomy in Compositional Reasoning: Scaling and Limitations of LLMs in Composite Task": [],
  "Subtractive Mixture Models via Squaring: Representation and Learning": [
    {
      "summary": "The authors consider the problem of learning a mixture model where the separate components do not have to be positive.\n\nThis can be naively done by squaring the additive MM but this is computationally inefficient.\n\nThe authors develop a method based on probabilistic circuits allowing to square different model structures without excessive computational cost.",
      "strengths": "The paper is reasonably clear and proposes simple yet interesting idea which appears to work well on selected synthetic/small scale experiments.\n\nThe authors provide the code for the experiments (which I did not reviewed).\n\nThe figure in page 1 nicely summarises the benefit of relaxing the requirement of positive components. Overall the figures in the paper help to understand the introduced concepts.\n\nI think the paper is an interesting read.",
      "weaknesses": "The clarity of the paper in pages 4,5,6 could be improved, the presentation is very dense and discusses multiple threads. The paper would benefit from focusing on core ideas and describing them in more detail while the less important parts could be moved to the appendix.\n\nI have concerns that a few points in the paper are overselling the method (i.e the result in Fig 5. on test data appears very small if statistically significant at all but using ^2 introduces additional computational cost). I would welcome the balanced discussion describing advantages and disadvantages of the method.\n\nThe authors do not discuss in detail how much additional computational cost is needed to achieve these results (a plot log-likelihood improvement vs CPU time would make the paper stronger). \n\nError bars in Figure 2 would help to understand the significance of empirical results.\n\nIn my eyes, the empirical improvements warranted by the proposed method are rather small and mostly shown on synthetic data.\n\nThe authors somewhat addressed three different questions in the empirical section but I feel it would be nicer to provide strong evidence for just one question: Does NPC^2 provide strong gains in performance without substantial increase in computational cost?\n\nIn Figure 4 the authors show that while NPC^2 outperforms MPC for LT and BT separately, for the cross comparison NPC^2(LT) vs MPC(BT) the latter can be better (similarly in table F5). This begs the question: is the RG doing the heavy lifting? If so, more empirical analysis would be helpful.\n\nThe authors should also elaborate on the selection of RG for improved clarity.\n\nWhy the differences reported in F2(a) are so small?\n\nCan the authors elaborate on the statement 105-106 regarding batching? I appears not fully clear to me. I cannot see how one can perform batching in (4) without introducing the bias to the gradient due to the presence of $\\nabla log Z$. Normally calculating  $\\nabla log Z$ requires sampling from the mode with every update of the parameters; it would be useful to provide exact update rule for clarity. Is the learning rule unbiased?\n\nSince $\\log c(x)^2 = 2 \\log c(x)$ the majority of the difference between maximising MM and MM^2 comes from the difference in the gradients of $\\log Z$ for MM and MM^2, I think this requires more clarity/explanation."
    },
    {
      "summary": "Motivated by mixture models, the paper investigates a class of functions that is a squared mixture of arbitrary functions with potentially negative weights and functions that do not necessarily represent a density function. After motivating this in the \"shallow\" regime, the authors propose extensions to deep mixtures based on tensorized circuits.",
      "strengths": "- [S1]: Originality -- as far as I can judge it's a novel and very interesting \n- [S2]: Significance -- seems to often work better than mixture models and other alternatives such as flows\n- [S3]: Clarity -- While the part related to tensor computations is a bit dense and could benefit from a more higher-level treatise, the paper is clearly written",
      "weaknesses": "- [W1]: Missing discussion / limitations: Maybe I overlooked this, but I could not find an actual discussion about the restriction of the approach, e.g., \n    + what are the limitations of the approach?\n    + how restrictive is the induced functional form by using squared functions?\n    + how expressive is the approach in the shallow or small-K setting? (Fig. 5 e.g. indicates that $\\pm$ is worse for small $K$)\n    + is it possible to extend the approach to a conditional setup?\n- [W2]: Experiments: The paper addresses the computational costs of the approach from a theoretical point of view and even provides some empirical evidence for the computation of the normalization constant, but empirically investigating a couple of scaling aspects such as \n    + the scaling in $D$ or \n    + a comparison with other appraoches such as the MAFs in terms of runtime \n\n    would provide further valuable insights (e.g. related to \"fairness\" when tuning different methods). \n- [W3]: Presentation (minor): Some of the graphics are rather hard to read:\n    + relatively small and dense (Fig. 4)\n    + the x or y-axis labels are sometimes missing (Fig. 4) or hard to find (not centered; Fig. 5, C1)"
    },
    {
      "summary": "Mixture models traditionally are represented as convex combinations of simpler probability distributions. This paper proposes loosening this constraint to any linear combination, and squaring the result at the end to ensure non-negativity. This modification is then applied to probabilistic circuits (by allowing negative weights in sums, and then squaring). Theoretical and empirical analysis confirm:\n\n1. Better distribution approximations for a given number of parameters (with a theoretical example showing exponential separation)\n2. Preservation of smoothness and decomposability when converting a PC to a squared NPC.",
      "strengths": "- Simple and effective idea\n- Empirical results show better performance than baseline on some tasks",
      "weaknesses": "- Paper's motivation can be stronger. e.g. add a real world motivating example. It would be interesting to see how the better density estimation can be used for an improved downstream task as well.\n- The NPCs use fewer parameters but in a more complex way. What is the impact of this on training cost. This question is not explored empirically."
    },
    {
      "summary": "Mixture models (herein called MPC) are probability density functions over a domain $X$ that are composed by adding together multiple simpler probability density functions. The authors propose a more general framework, (called NPC), where any functions over the domain $X$ (or specific dimensions of $X$) may be multiplied, added, subtracted together and finally squared to return a non-negative scalar which, under the right conditions the authors prove, can also be normalised yielding a probability density. Given the same number of parameters, the proposed NPC models have far more expressive capability than traditional MPCs, in fact proven exponentially more capacity.\n\nThe authors describe a range of concepts, rooted graphs, tensor circuits, to lead to the conditions for a given NPC to be marginalizable. Theoretical relationships with related work is described and finally experiments demonstrate the efficacy of the propoosed approach.",
      "strengths": "- extensive theoretical evaluation\n  - proof of conditions for normalization which significantly affects model design\n  - proof of exponential expresivity\n  - proofs of connections to related methods\n\n- simplicity\n  - take some functions over the space $X$, functions over disjoint subsets of dimensions\n  - multiply them (across disjoint groups)\n  - add them together (functions within a disjoint group)\n  - sum the output and square it to get a single non-negative scalar value\n  - if we followed the rules, we can find the partition function/normalising constant in one forward pass of the function\n  - I feel this is very intuitive and surprisingly simple, and most importantly  avoids difficult normalisation (e.g. MCMC) while yielding a big improvement in expressiveness over standard mixture models. Analytic normalisation also enables conditioning. Sampling is briefly mentioned in the Appendix and appears to be one point to be harder than MPCs.",
      "weaknesses": "While I feel confident I understood the method paper and the paper, I am not familiar with the surrounding literature hence my comments are mainly on the general practicalities and the paper presentation.\n\n## Major Comments\n\nThe main four big questions I have mostly relate to practical considerations  \n- How do I normalise $c(X)$? Much of section 3 is building foundations to finally reach proposition 1 which successfully solves the issue of normalization (which is a significant strength of the paper in my view)\n- For a given dataset, how do I find the rooted graph? The issue is not so carefully discussed, and presumably there is no theoretical or provable result and in practice one must simply employ some sort of architecture search. If not already, this should be clarified in the main paper as I feel it may be an obstacle for practitioners.\n- Given the exponentially more expressiveness, could this easily lead to overfitting? A quick word search in the document doesn't yield results, this is not mentioned once?\n- How do I generate samples? This is described in the appendix and appears to be not as easy compared to MPCs, \n\nI understand if the authors would like to argue this is a more theoretical paper and such practicalities are beyond scope.\n\n## Minor Comments (Presentation)\n\n### Content Density\nUpon first reading, I was exhausted by section 5, however upon second reading the paper made much more sense.\n  - Sections 1, 2 were simple and easy to follow\n  - section 3 was hard work on first reading but very clear on second reading (see details below)\n  - section 4 contains multiple short sharp deep dives into a range of related fields.\n  - section 5 was very short and I personally didn't truly understand the benchmarks nor get a feel for implementing the method or its practicalities or failure modes, (e.g. overfitting, sensitivity to rooted graph, sampling)\n\nGiven the main paper introduces a range of concepts, then proposes a new method, provides proven results and proves connections to related fields and then benchmarks, I feel like this is a (very nice) journal paper that was heavily compressed into 9 pages and all of the overflow was placed in the appendix. \n\nThe theoretical treatment is extensive. The authors may consider moving some of the less significant content regarding other works in Section 4 to the appendix in order to extend section 5 with more \"hands on\" details about the using NPCs, e.g. a worked illustrative example or failure modes or sensitivity to the rooted graph.\n\n### Section 3 detailed comments\nI understand computational graphs have nodes that are operations and the links are tensors, \n- Definition 1 was very hard to follow, the ambiguous notation that $\\ell$ represent a layer as well as a numerical output tensor from a layer. \n- Figure 2 b, c, blur the boundary of \"nodes\" and \"edges\", there are rectangles with operations, and there are volumes between them also with operations (hadamard product and $W$). Even now I struggle to parse these diagrams."
    },
    {
      "summary": "The paper proposes a way to learn and represent mixture models with negative weights by squaring the linear combination defined by these models. The resulting model is cast into the Probabilistic Circuits (PCs) framework and thus further extended to deep mixture models, resulting in the main contribution of the paper, squared non-monotonic PCs (or NPC$^2$s). NPC$^2$s are theoretically proven to be more expressive efficient than regular (or monotonic) PCs, which translates to strong empirical results in a number of benchmarks.",
      "strengths": "The work is novel and certainly relevant for the Tractable Probabilistic Models community, since it adds a new and provenly more expressive model to the class of Probabilistic Circuits. More importantly, the main ideas are well developed in the text and thoroughly analyzed both theoretically and empirically, allowing for a well-rounded understanding of this new class of models. The text itself is well written and easy to follow, and the related work develops important connections to other methods and models in the literature of Probabilistic Circuits and beyond.",
      "weaknesses": "The paper is very well executed, and honestly I cannot think of any major flaws or possible improvements besides the couple of questions I outline below."
    }
  ],
  "CABINET: Content Relevance-based Noise Reduction for Table Question Answering": [
    {
      "summary": "To minimize the negative effects from the noisy and distracting information from irrelevant parts of the table, in this work, the authors propose CABINET framework that weighs different table parts based on their relevance to the question without explicitly removing any content. The main part of this framework is on the relevance score assignment, either with unsupervised learning or leveraging external weakly-supervised cell highlighter model, which provides the relevance measurement between table content and question. \nSpecifically, variational inference is used to obtain the relevance score with unsupervised fashion. Furthermore, CABINET leverages a parsing statement generator that describes which rows and columns are relevant to the question to provide more matching information. Experimental results show that: (1) The relevance score with three designed loss functions can improve the model performance. (2) The relevance scores obtained from unsupervised setting and weakly-supervised setting should be balanced to obtain good performance.",
      "strengths": "The model achieves the state-of-the-art performance on the WikiTQ, FetaQA and WikiSQL using orders of magnitude fewer parameters when compared of LLMs such as Codex. \nFurthermore, the model is more robust on the perturbation than other counterparts. Also for large tables, the CABINET also shows better performance than OmniTab.  \nThe authors smartly use the ToTTo dataset to train a cell highlighter model for obtaining the relevance score from the parsing statement. \nThe ~300 manually annotated example for parsing statement generation will be very useful for the community.",
      "weaknesses": "Even though there is a good ablation experiments on the three loss functions, my overall feeling is that there is no good rational how these loss function interact with each other. For example, we can see that combining all three loss functions performs best. However, the benefits of a single loss function is hard to be observed. I think this strategy can be further applied to reading comprehension task. If it works, this can make the paper more comprehensive, having broader impact. No other weaknesses in my mind but there are some technical questions in Question section.\n\n-----\nWith responses and updated number for reading comprehension task, I am happy to change the rating accordingly."
    },
    {
      "summary": "This paper introduces a novel architecture for table QA. It is with two components, which are the main contributions of this paper. Unsupervised Relevance Scorer provides a \"soft\" assignment score for each row of the table given the question. Relevant Cell Predictor through Table Parsing converts the question into a statement with highlighted column and row information that can be used to solve the QA tasks. This method achieves SOTA performances on WikiTable, FeTaQA, and WikiSQL. Overall, this is a great paper.",
      "strengths": "* This paper proposed a novel framework for table QA tasks\n* The proposed method achieves SOTA performances on three table QA tasks\n* The paper shows the robustness of the provided model",
      "weaknesses": "* Unlike a \"hard\" table token retriever, the computation of the model proposed by this paper may be extensive when the number of table tokens is many. \n* More ablation studies are needed to support the needs of the components introduced in this paper."
    },
    {
      "summary": "This paper tackles the task of table Question-Answering (QA). The authors state that the approaches considering all the tables have too much noise when generating the answer, while approaches selecting a part of the table before answering might remove the relevant parts. The authors thus propose to estimate the relevance of each token based on a clustering approach with latent variable (Srivastava and Sutton, 2017), coupled with two custom losses (sparsity of the relevance cluster and distance between the centroids). They combine this token relevance score with a predictor of a table \"cell relevance\" based on a sequence-to-sequence model that outputs the relevant cells from a full table. \n\nResults reported on the main table QA datasets (WikiTableQuestion, FeTaQa, and WikiSQL) show a noticeable improvement (e.g. gaining 3 points in accuracy compared to a LLM/Codex prompting approach that select subtables, DATER, on WikiTQ). Experiments consisting in perturbing the tables (adding rows, column/row permutation, cell replacement) show that the method is more robust than OmniTAB and ReasTAP. Finally,",
      "strengths": "- The approach has good results on the main table QA dataset\n- The ablation study shows that each subcomponent of the model is important\n- The model outperforms LLM-based approaches with a much lower number of parameters (175B vs 3.5B) and other state-of-the-art approaches (e.g. OmniTAB)",
      "weaknesses": "- The overall model is quite complex, relying on various subcomponents of different natures: this paper provides a new baseline, but it is hard to build from that\n- The cell relevance loss is not learned, making the model not end-to-end\n- Using a latent variable for relevance, which is optimized as within a probabilistic model for relevance, but then used as a scalar, makes the model a bit inconsistent - a fully probabilistic model would have been much sounder\n- there are missing experimental details"
    },
    {
      "summary": "The authors propose CABINET (Content RelevAnce-Based NoIse ReductioN for TablE QuesTion-Answering) \u2013 a framework to enable LLMs to focus on relevant tabular data by suppressing extraneous information. CABINET comprises an Unsupervised Relevance Scorer, trained differentially with the QA LLM, that weighs the table content based on its relevance to the input question before feeding it to the question-answering LLM (QA LLM). Further, it uses a weakly supervised module that generates a parsing statement describing the criteria of rows and columns relevant to the question and highlights the content of corresponding table cells. Authors release code and dataset to enable for reproducibility.",
      "strengths": "The paper is technically sound and in general well written. Figure 2 is also informative.",
      "weaknesses": "- The authors need to better motivate what the practical utility of Table QA is, as most of the questions are based in the format of natural language text sentences. Moreover, tabular dataset can be indexed and converted to their freeform text representations which general open-domain QA systems are already able to solve. \n\n- In the Related Work Section, the authors should also discuss recent developments with LLM guided graph neural network (GNN) model for QA to capture tabular/graph structure:\n\n[1] EACL 2023: Question-Answer Sentence Graph for Joint Modeling Answer Selection. In Proceedings of the 17th Conference of the European Chapter of the Association for Computational Linguistics, pages 968\u2013979, Dubrovnik, Croatia. Association for Computational Linguistics.\n[2] AAAI 2020: TANDA: Transfer and Adapt Pre-Trained Transformer Models for Answer Sentence Selection. AAAI 2020: 7780-7788\n\n- It would be helpful if the authors could better summarize the statistics of their datasets in tabular as opposed to text format. Further, based on the text description, I have a concern that the datasets being used are relatively small-scale, e.g., in the order of thousands of nodes. The experiment results would be more conclusive if evaluated on large-scale datasets. To this end, the authors also should provide analysis on the runtime and memory complexities of their work, since M/B parameter LLMs may not be scalable for training time."
    }
  ],
  "Improving equilibrium propagation without weight symmetry through Jacobian homeostasis": [
    {
      "summary": "Equilibrium propagation, an alternative to backprop requires weight symmetry and nudges to yield unbiased gradient estimates. Generalization of Equilibrium proportion to non-symmetric dynamical system exists but shown to work only for simple problems.  \nThe paper proposes an extension of holomorphic EP to non-symmetrical dynamical systems and shows good results across different vision benchmarks.",
      "strengths": "- Analysis of bias in gradient estimation is helpful to the reader. \n- Incorporating functional symmetry through the use of matching jacobians is an interesting idea. It\u2019s similar to reconstruction error term used in methods like Target Propagation. Here, authors optimize the homeostatic loss with respect to all the weights as compared to using only feedback weights.",
      "weaknesses": "- The paper is generally well written, though introduction is a bit complex if the reader is not aware of previous work (holomorphic EP)."
    },
    {
      "summary": "_Disclaimer: I have reviewed this paper recently again at a different venue. It has since been revised slightly. Some of my comments still apply to this version and are copied here verbatim from my previous review._\n\nThe paper focuses on Equilbrium Propagation (EP), i.e. a learning algorithm for neural networks that was introduced relatively recently and aspires to be more biologically plausible and more suitable for analog hardware than back-propagation (BP). The theoretical version of EP relies on an infinitesimal perturbation (which is not feasible to achieve in physical settings) and on symmetric weights (which are also unrealistic in biology and constrain hardware design). The paper aims to understand the impact of each of these two issues separately. It achieves to do so in the \"holomorphic EP\" (hEP) setting, i.e. under the assumption of complex-valued neurons. The authors achieve this by extending the theory of hEP to asymmetric connectivity, where they show that it is possible to obtain exact estimates of the gradient despite finite perturbations, therefore suggesting that the bigger issue is weight asymmetry. They then proceed to tackle the problem of asymmetry by introducing a loss term that penalizes it directly, and they show experimentally that this term improves the performance of hEP significantly in asymmetrically-initialized networks, in certain visual tasks using a 4-layer convolutional network.",
      "strengths": "The paper is mostly nicely written and clear. The theoretical contributions of the paper are not trivial, as they require a deep understanding of a very specific algorithm i.e. the holomorphic version of EP, as well as a degree of comfort with certain mathematical concepts that is rare among neural network practitioners and possibly even theoreticians. More generally, the paper aims to contribute to an area that is of broad interest, as it relates to machine learning, neuromorphic engineering, and theoretical neuroscience. Furthermore, it truly advances the empirical results of the EP-related literature.",
      "weaknesses": "While this is obviously a valuable piece of work in certain respects, I believe it is also significantly limited in other key aspects.\n\nSignificance: The work ultimately aspires to improve aspects of backpropagation that are indeed important and relevant to multiple large disciplines (ML, neuromorphic hardware, neuroscience), however, concretely, the resulting contribution is very narrow. Namely, it improves the performance and the theoretical understanding of hEP somewhat, specifically in the case of asymmetric weights, but it doesn't resolve the problem of asymmetry completely, as can be seen in the performance comparison to symmetric weights (Table 2). Moreover, hEP is a specialized version of EP that makes additional assumptions for complex-valued networks, which limits the applicability and generality of the algorithm. Furthermore, EP itself more broadly is interesting but is a rather limited method in terms of achieving its goals of good performance, efficiency, useful hardware demonstrations, or deep learning, in comparison with other methods that have similar goals that have been more successful. Its biological compatibility could also be debated, given its requirement for multiple network-wide iterations before a weight update. Therefore, it could be argued that the significance of the results only relates to a very narrow subfield. Furthermore, the theoretical advance separating the impact of the finite nudge from that of the asymmetry in hEP is not trivial, but I wonder how useful it is to the ICLR community, beyond the very narrow sub-community that specializes in hEP. In the broader picture, and given the already existing better-working alternatives, the progress made here towards biological plausibility or neuromorphic computation seems small, in my view.\n\nNovelty: The main novelty of the paper is in the theoretical results, since the empirical fact that improving performance by dealing with asymmetric weights is not a new result. Nevertheless, I suspect that to someone with a good understanding of the earlier mathematical work that introduced hEP (Laborieux & Zenke, NeurIPS 2022) the new theoretical results might not be very surprising. However, it should be noted that I am not in a position to judge this fully. In any case, the insight from the theory is the impact of asymmetrical weights, relative to that of finite nudge, which seems to be also covered by the empirical result, therefore the added value from the theory in this case might not be substantial. Regarding the paper's empirical or practical contributions, some novelty exists in the objective function that penalizes asymmetries in the weight matrix. However, the novelty of this is limited because much older learning rules that achieve weight symmetry do exist (Kolen & Polak, IJCNN 1994; see also Payeur et al., Nat. Neurosci. 2021). In fact, these rules do not rely on global iterative equilibria, so the present paper's implementation of learnable weight symmetry could be characterized as a step-back in this regard. It should be recognize though that the new method is applicable when the connectivity is not reciprocal, whereas previous ones probably were not.\n\nContextualization in the literature: The paper does (now) cite some of the works that had similar aims, but only in passing, only in the discussion as opposed to the motivation section, discounts the better empirical results that the other works achieved, attributing this only to the lower simulation cost of the alternatives, and does not mention that some of these methods not only perform better in classification benchmarks, but also require fewer assumptions for compatibility with biology and for efficiency in learning hardware, e.g. by circumventing the need for backward passes of information completely.\nSome examples are: Payeur et al., Nat. Neurosci. 2021; Greedy et al., NeurIPS 2022; Mengye Ren et al., ICLR 2023; Journ\u00e9 et al., ICLR 2023.\n\nIt would be very helpful to the readers and the targeted research communities if the authors motivated their choice to focus specifically on EP as opposed to alternative methods that have similar goals, but don't have the same limitations. This could be a way to mitigate the weaknesses of the paper to some extent.\n\nTo be clear, some of the algorithm's limitations in comparison to alternatives are: reliance on complex-valued networks; constraints in network architecture and depth, expensive simulations for the equilibrium dynamics (e.g. a multi-GPU cluster was used according to the supplementary material, despite the small networks and simple tasks); questionable bio-plausibility of the necessity for equilibrium; hardware implementations of EP are mostly theoretical. To their credit, the authors have mentioned some of these limitations in the discussion. They also provided some solutions or counterarguments, however these are largely theoretical, vague, or speculative.\n\nAll in all, I believe that the work's value might be able to increase if the manuscript could explain its motivations and its contributions in a context broader than the EP literature. At present, this is not clear enough or supported well enough, in my view."
    },
    {
      "summary": "This paper studied the weight asymmetry in the weight transport problem extending the Holomorphic Equilibrium Propagation algorithm (hEP), through investigations about the Jacobian of the network, with experiments and theoretically analysed the reasons for the experimental results. Based on the analysis results, some new features were proposed to enable the algorithm to evade the need for perfect weight symmetry without affecting the functional symmetry. A new form of Jacobian homeostasis was introduced to maintain functional symmetry, without directly addressing weight symmetry. Finally, several experiments, including the investigation of weight symmetry evolution during training and comparative experiments, were conducted to verify the effectiveness of the proposed algorithm, and the work\u2019s performance exceeded the networks with Recurrent Backpropagation (RBP), even on larger datasets.",
      "strengths": "*Relatively new perspective\n\nThis paper shows a new perspective on studying the weight transport problem with the Jacobian of the network. It explains the connections between weight symmetry, functional symmetry and Jacobian homeostasis.\n\n*Relatively rigorous analysis and persuasive experiments\n\nThis paper shows the efforts in investigating the weight symmetry evolution during training in section 4. If the observations could be discussed more deeply with the figure would be better. \n\n*The part about \u2018Jacobian homeostasis improves functional symmetry\u2019 in section 4 is very detailed and well analysed.",
      "weaknesses": "*Some expressions with flaws\n\nAlthough \u2018hEP\u2019 is the abbreviation of \u2018holomorphic Equilibrium propagation\u2019 can be understood after reading. However, this abbreviation has not been indicated in parenthesis when mentioning its full term for the first time.\nThe last sentence \u2018It is worth nothing that\u2026\u2019, in Definition 1 of section 2.2, might be miswritten, which should be \u2018It is worth noting that\u2026\u2019 based on the context. Also in this paragraph, the sentence after Eq. (6) \u2018Importantly, the quantities \u2026, which applies only to EBMs.\u2019, is a little longer and complex. It might confuse readers, for convenience of understanding, it would be better to split it into two simpler sentences.\n\n*Some disadvantages in the layout\n\nIn section 3.2, the paragraph with Eq. (9) is intersected by Figure 2, which could be rearranged to provide enhanced readability. The same disadvantages happen in Figure 3 and Table 1.\n\n*The performance of the proposed method is relatively not so advanced. And the method for comparison is relatively not so new, making the work of this article not very convincing."
    },
    {
      "summary": "The authors suggest a modification to the holomorphic equilibrium propagation algorithm, that helps it deal with cases where the Jacobian is not symmetric, by adding a term which penalizes asymmetry of the Jacobian. They show this approach outperforms standard holomorphic equilibrium propagation.",
      "strengths": "The method improves over previous holomorphic equilibrium propagation in the applications tested. \n\nAs someone who is not in this field, I found section 2 to be informative introduction to the area.",
      "weaknesses": "It was not entirely clear to me what the motivation for this work is.  While the authors suggest that hEP is potentially biologically plausible, this seems like a stretch.  Not only does it require neurons to do computations with complex numbers, it also seems to require that the network settle to equilibrium at multiple phases of an ongoing oscillation.  This does not seem likely in the brain: the period of the gamma oscillation is tens of milliseconds, which certainly would not allow enough time, and even the theta oscillation seems to fast for this. It also was not clear what computational advantages this might lead to in purely artificial systems for which biological plausibility was not important."
    }
  ]
}